/scratch/nsk367/anaconda3/envs/vit/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO:lightning:GPU available: True, used: True
INFO:lightning:CUDA_VISIBLE_DEVICES: [0]
INFO:lightning:Set SLURM handle signals.
INFO:lightning:
    | Name                                                    | Type        | Params
------------------------------------------------------------------------------------
0   | _ViT_Trainer__model                                     | ViT         | 25 M  
1   | _ViT_Trainer__model.to_patch_embedding                  | Sequential  | 25 K  
2   | _ViT_Trainer__model.to_patch_embedding.0                | Rearrange   | 0     
3   | _ViT_Trainer__model.to_patch_embedding.1                | Linear      | 25 K  
4   | _ViT_Trainer__model.dropout                             | Dropout     | 0     
5   | _ViT_Trainer__model.transformer                         | Transformer | 25 M  
6   | _ViT_Trainer__model.transformer.layers                  | ModuleList  | 25 M  
7   | _ViT_Trainer__model.transformer.layers.0                | ModuleList  | 2 M   
8   | _ViT_Trainer__model.transformer.layers.0.0              | PreNorm     | 1 M   
9   | _ViT_Trainer__model.transformer.layers.0.0.norm         | LayerNorm   | 1 K   
10  | _ViT_Trainer__model.transformer.layers.0.0.fn           | Attention   | 1 M   
11  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_qkv    | Linear      | 1 M   
12  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out    | Sequential  | 393 K 
13  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out.0  | Linear      | 393 K 
14  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out.1  | Dropout     | 0     
15  | _ViT_Trainer__model.transformer.layers.0.1              | PreNorm     | 526 K 
16  | _ViT_Trainer__model.transformer.layers.0.1.norm         | LayerNorm   | 1 K   
17  | _ViT_Trainer__model.transformer.layers.0.1.fn           | FeedForward | 525 K 
18  | _ViT_Trainer__model.transformer.layers.0.1.fn.net       | Sequential  | 525 K 
19  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.0     | Linear      | 262 K 
20  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.1     | GELU        | 0     
21  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.2     | Dropout     | 0     
22  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.3     | Linear      | 262 K 
23  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.4     | Dropout     | 0     
24  | _ViT_Trainer__model.transformer.layers.1                | ModuleList  | 2 M   
25  | _ViT_Trainer__model.transformer.layers.1.0              | PreNorm     | 1 M   
26  | _ViT_Trainer__model.transformer.layers.1.0.norm         | LayerNorm   | 1 K   
27  | _ViT_Trainer__model.transformer.layers.1.0.fn           | Attention   | 1 M   
28  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_qkv    | Linear      | 1 M   
29  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out    | Sequential  | 393 K 
30  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out.0  | Linear      | 393 K 
31  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out.1  | Dropout     | 0     
32  | _ViT_Trainer__model.transformer.layers.1.1              | PreNorm     | 526 K 
33  | _ViT_Trainer__model.transformer.layers.1.1.norm         | LayerNorm   | 1 K   
34  | _ViT_Trainer__model.transformer.layers.1.1.fn           | FeedForward | 525 K 
35  | _ViT_Trainer__model.transformer.layers.1.1.fn.net       | Sequential  | 525 K 
36  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.0     | Linear      | 262 K 
37  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.1     | GELU        | 0     
38  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.2     | Dropout     | 0     
39  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.3     | Linear      | 262 K 
40  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.4     | Dropout     | 0     
41  | _ViT_Trainer__model.transformer.layers.2                | ModuleList  | 2 M   
42  | _ViT_Trainer__model.transformer.layers.2.0              | PreNorm     | 1 M   
43  | _ViT_Trainer__model.transformer.layers.2.0.norm         | LayerNorm   | 1 K   
44  | _ViT_Trainer__model.transformer.layers.2.0.fn           | Attention   | 1 M   
45  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_qkv    | Linear      | 1 M   
46  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out    | Sequential  | 393 K 
47  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out.0  | Linear      | 393 K 
48  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out.1  | Dropout     | 0     
49  | _ViT_Trainer__model.transformer.layers.2.1              | PreNorm     | 526 K 
50  | _ViT_Trainer__model.transformer.layers.2.1.norm         | LayerNorm   | 1 K   
51  | _ViT_Trainer__model.transformer.layers.2.1.fn           | FeedForward | 525 K 
52  | _ViT_Trainer__model.transformer.layers.2.1.fn.net       | Sequential  | 525 K 
53  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.0     | Linear      | 262 K 
54  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.1     | GELU        | 0     
55  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.2     | Dropout     | 0     
56  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.3     | Linear      | 262 K 
57  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.4     | Dropout     | 0     
58  | _ViT_Trainer__model.transformer.layers.3                | ModuleList  | 2 M   
59  | _ViT_Trainer__model.transformer.layers.3.0              | PreNorm     | 1 M   
60  | _ViT_Trainer__model.transformer.layers.3.0.norm         | LayerNorm   | 1 K   
61  | _ViT_Trainer__model.transformer.layers.3.0.fn           | Attention   | 1 M   
62  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_qkv    | Linear      | 1 M   
63  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out    | Sequential  | 393 K 
64  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out.0  | Linear      | 393 K 
65  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out.1  | Dropout     | 0     
66  | _ViT_Trainer__model.transformer.layers.3.1              | PreNorm     | 526 K 
67  | _ViT_Trainer__model.transformer.layers.3.1.norm         | LayerNorm   | 1 K   
68  | _ViT_Trainer__model.transformer.layers.3.1.fn           | FeedForward | 525 K 
69  | _ViT_Trainer__model.transformer.layers.3.1.fn.net       | Sequential  | 525 K 
70  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.0     | Linear      | 262 K 
71  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.1     | GELU        | 0     
72  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.2     | Dropout     | 0     
73  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.3     | Linear      | 262 K 
74  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.4     | Dropout     | 0     
75  | _ViT_Trainer__model.transformer.layers.4                | ModuleList  | 2 M   
76  | _ViT_Trainer__model.transformer.layers.4.0              | PreNorm     | 1 M   
77  | _ViT_Trainer__model.transformer.layers.4.0.norm         | LayerNorm   | 1 K   
78  | _ViT_Trainer__model.transformer.layers.4.0.fn           | Attention   | 1 M   
79  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_qkv    | Linear      | 1 M   
80  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out    | Sequential  | 393 K 
81  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out.0  | Linear      | 393 K 
82  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out.1  | Dropout     | 0     
83  | _ViT_Trainer__model.transformer.layers.4.1              | PreNorm     | 526 K 
84  | _ViT_Trainer__model.transformer.layers.4.1.norm         | LayerNorm   | 1 K   
85  | _ViT_Trainer__model.transformer.layers.4.1.fn           | FeedForward | 525 K 
86  | _ViT_Trainer__model.transformer.layers.4.1.fn.net       | Sequential  | 525 K 
87  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.0     | Linear      | 262 K 
88  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.1     | GELU        | 0     
89  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.2     | Dropout     | 0     
90  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.3     | Linear      | 262 K 
91  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.4     | Dropout     | 0     
92  | _ViT_Trainer__model.transformer.layers.5                | ModuleList  | 2 M   
93  | _ViT_Trainer__model.transformer.layers.5.0              | PreNorm     | 1 M   
94  | _ViT_Trainer__model.transformer.layers.5.0.norm         | LayerNorm   | 1 K   
95  | _ViT_Trainer__model.transformer.layers.5.0.fn           | Attention   | 1 M   
96  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_qkv    | Linear      | 1 M   
97  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out    | Sequential  | 393 K 
98  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out.0  | Linear      | 393 K 
99  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out.1  | Dropout     | 0     
100 | _ViT_Trainer__model.transformer.layers.5.1              | PreNorm     | 526 K 
101 | _ViT_Trainer__model.transformer.layers.5.1.norm         | LayerNorm   | 1 K   
102 | _ViT_Trainer__model.transformer.layers.5.1.fn           | FeedForward | 525 K 
103 | _ViT_Trainer__model.transformer.layers.5.1.fn.net       | Sequential  | 525 K 
104 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.0     | Linear      | 262 K 
105 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.1     | GELU        | 0     
106 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.2     | Dropout     | 0     
107 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.3     | Linear      | 262 K 
108 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.4     | Dropout     | 0     
109 | _ViT_Trainer__model.transformer.layers.6                | ModuleList  | 2 M   
110 | _ViT_Trainer__model.transformer.layers.6.0              | PreNorm     | 1 M   
111 | _ViT_Trainer__model.transformer.layers.6.0.norm         | LayerNorm   | 1 K   
112 | _ViT_Trainer__model.transformer.layers.6.0.fn           | Attention   | 1 M   
113 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_qkv    | Linear      | 1 M   
114 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out    | Sequential  | 393 K 
115 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out.0  | Linear      | 393 K 
116 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out.1  | Dropout     | 0     
117 | _ViT_Trainer__model.transformer.layers.6.1              | PreNorm     | 526 K 
118 | _ViT_Trainer__model.transformer.layers.6.1.norm         | LayerNorm   | 1 K   
119 | _ViT_Trainer__model.transformer.layers.6.1.fn           | FeedForward | 525 K 
120 | _ViT_Trainer__model.transformer.layers.6.1.fn.net       | Sequential  | 525 K 
121 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.0     | Linear      | 262 K 
122 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.1     | GELU        | 0     
123 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.2     | Dropout     | 0     
124 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.3     | Linear      | 262 K 
125 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.4     | Dropout     | 0     
126 | _ViT_Trainer__model.transformer.layers.7                | ModuleList  | 2 M   
127 | _ViT_Trainer__model.transformer.layers.7.0              | PreNorm     | 1 M   
128 | _ViT_Trainer__model.transformer.layers.7.0.norm         | LayerNorm   | 1 K   
129 | _ViT_Trainer__model.transformer.layers.7.0.fn           | Attention   | 1 M   
130 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_qkv    | Linear      | 1 M   
131 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out    | Sequential  | 393 K 
132 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out.0  | Linear      | 393 K 
133 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out.1  | Dropout     | 0     
134 | _ViT_Trainer__model.transformer.layers.7.1              | PreNorm     | 526 K 
135 | _ViT_Trainer__model.transformer.layers.7.1.norm         | LayerNorm   | 1 K   
136 | _ViT_Trainer__model.transformer.layers.7.1.fn           | FeedForward | 525 K 
137 | _ViT_Trainer__model.transformer.layers.7.1.fn.net       | Sequential  | 525 K 
138 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.0     | Linear      | 262 K 
139 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.1     | GELU        | 0     
140 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.2     | Dropout     | 0     
141 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.3     | Linear      | 262 K 
142 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.4     | Dropout     | 0     
143 | _ViT_Trainer__model.transformer.layers.8                | ModuleList  | 2 M   
144 | _ViT_Trainer__model.transformer.layers.8.0              | PreNorm     | 1 M   
145 | _ViT_Trainer__model.transformer.layers.8.0.norm         | LayerNorm   | 1 K   
146 | _ViT_Trainer__model.transformer.layers.8.0.fn           | Attention   | 1 M   
147 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_qkv    | Linear      | 1 M   
148 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_out    | Sequential  | 393 K 
149 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_out.0  | Linear      | 393 K 
150 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_out.1  | Dropout     | 0     
151 | _ViT_Trainer__model.transformer.layers.8.1              | PreNorm     | 526 K 
152 | _ViT_Trainer__model.transformer.layers.8.1.norm         | LayerNorm   | 1 K   
153 | _ViT_Trainer__model.transformer.layers.8.1.fn           | FeedForward | 525 K 
154 | _ViT_Trainer__model.transformer.layers.8.1.fn.net       | Sequential  | 525 K 
155 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.0     | Linear      | 262 K 
156 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.1     | GELU        | 0     
157 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.2     | Dropout     | 0     
158 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.3     | Linear      | 262 K 
159 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.4     | Dropout     | 0     
160 | _ViT_Trainer__model.transformer.layers.9                | ModuleList  | 2 M   
161 | _ViT_Trainer__model.transformer.layers.9.0              | PreNorm     | 1 M   
162 | _ViT_Trainer__model.transformer.layers.9.0.norm         | LayerNorm   | 1 K   
163 | _ViT_Trainer__model.transformer.layers.9.0.fn           | Attention   | 1 M   
164 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_qkv    | Linear      | 1 M   
165 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_out    | Sequential  | 393 K 
166 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_out.0  | Linear      | 393 K 
167 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_out.1  | Dropout     | 0     
168 | _ViT_Trainer__model.transformer.layers.9.1              | PreNorm     | 526 K 
169 | _ViT_Trainer__model.transformer.layers.9.1.norm         | LayerNorm   | 1 K   
170 | _ViT_Trainer__model.transformer.layers.9.1.fn           | FeedForward | 525 K 
171 | _ViT_Trainer__model.transformer.layers.9.1.fn.net       | Sequential  | 525 K 
172 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.0     | Linear      | 262 K 
173 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.1     | GELU        | 0     
174 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.2     | Dropout     | 0     
175 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.3     | Linear      | 262 K 
176 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.4     | Dropout     | 0     
177 | _ViT_Trainer__model.transformer.layers.10               | ModuleList  | 2 M   
178 | _ViT_Trainer__model.transformer.layers.10.0             | PreNorm     | 1 M   
179 | _ViT_Trainer__model.transformer.layers.10.0.norm        | LayerNorm   | 1 K   
180 | _ViT_Trainer__model.transformer.layers.10.0.fn          | Attention   | 1 M   
181 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_qkv   | Linear      | 1 M   
182 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_out   | Sequential  | 393 K 
183 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_out.0 | Linear      | 393 K 
184 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_out.1 | Dropout     | 0     
185 | _ViT_Trainer__model.transformer.layers.10.1             | PreNorm     | 526 K 
186 | _ViT_Trainer__model.transformer.layers.10.1.norm        | LayerNorm   | 1 K   
187 | _ViT_Trainer__model.transformer.layers.10.1.fn          | FeedForward | 525 K 
188 | _ViT_Trainer__model.transformer.layers.10.1.fn.net      | Sequential  | 525 K 
189 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.0    | Linear      | 262 K 
190 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.1    | GELU        | 0     
191 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.2    | Dropout     | 0     
192 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.3    | Linear      | 262 K 
193 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.4    | Dropout     | 0     
194 | _ViT_Trainer__model.transformer.layers.11               | ModuleList  | 2 M   
195 | _ViT_Trainer__model.transformer.layers.11.0             | PreNorm     | 1 M   
196 | _ViT_Trainer__model.transformer.layers.11.0.norm        | LayerNorm   | 1 K   
197 | _ViT_Trainer__model.transformer.layers.11.0.fn          | Attention   | 1 M   
198 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_qkv   | Linear      | 1 M   
199 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_out   | Sequential  | 393 K 
200 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_out.0 | Linear      | 393 K 
201 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_out.1 | Dropout     | 0     
202 | _ViT_Trainer__model.transformer.layers.11.1             | PreNorm     | 526 K 
203 | _ViT_Trainer__model.transformer.layers.11.1.norm        | LayerNorm   | 1 K   
204 | _ViT_Trainer__model.transformer.layers.11.1.fn          | FeedForward | 525 K 
205 | _ViT_Trainer__model.transformer.layers.11.1.fn.net      | Sequential  | 525 K 
206 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.0    | Linear      | 262 K 
207 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.1    | GELU        | 0     
208 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.2    | Dropout     | 0     
209 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.3    | Linear      | 262 K 
210 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.4    | Dropout     | 0     
211 | _ViT_Trainer__model.to_latent                           | Identity    | 0     
212 | _ViT_Trainer__model.mlp_head                            | Sequential  | 6 K   
213 | _ViT_Trainer__model.mlp_head.0                          | LayerNorm   | 1 K   
214 | _ViT_Trainer__model.mlp_head.1                          | Linear      | 5 K   
Files already downloaded and verified
Files already downloaded and verified
Validation sanity check: 0it [00:00, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Validation sanity check:  20%|██        | 1/5 [00:03<00:15,  3.77s/it]Validation sanity check:  40%|████      | 2/5 [00:03<00:04,  1.63s/it]Validation sanity check:  60%|██████    | 3/5 [00:04<00:01,  1.06it/s]Validation sanity check:  80%|████████  | 4/5 [00:04<00:00,  1.62it/s]Validation sanity check: 100%|██████████| 5/5 [00:04<00:00,  2.27it/s]                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/196 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s] /scratch/nsk367/anaconda3/envs/vit/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 1:   1%|          | 1/196 [00:01<04:24,  1.36s/it]Epoch 1:   1%|          | 1/196 [00:01<04:24,  1.36s/it, loss=2.439, v_num=6664321]Epoch 1:   1%|          | 2/196 [00:01<02:47,  1.16it/s, loss=2.439, v_num=6664321]Epoch 1:   1%|          | 2/196 [00:01<02:47,  1.16it/s, loss=4.280, v_num=6664321]Epoch 1:   2%|▏         | 3/196 [00:02<02:14,  1.43it/s, loss=4.280, v_num=6664321]Epoch 1:   2%|▏         | 3/196 [00:02<02:14,  1.43it/s, loss=4.569, v_num=6664321]Epoch 1:   2%|▏         | 4/196 [00:02<01:58,  1.62it/s, loss=4.569, v_num=6664321]Epoch 1:   2%|▏         | 4/196 [00:02<01:58,  1.62it/s, loss=4.490, v_num=6664321]Epoch 1:   3%|▎         | 5/196 [00:02<01:48,  1.77it/s, loss=4.490, v_num=6664321]Epoch 1:   3%|▎         | 5/196 [00:02<01:48,  1.77it/s, loss=4.362, v_num=6664321]Epoch 1:   3%|▎         | 6/196 [00:03<01:41,  1.87it/s, loss=4.362, v_num=6664321]Epoch 1:   3%|▎         | 6/196 [00:03<01:41,  1.87it/s, loss=4.181, v_num=6664321]Epoch 1:   4%|▎         | 7/196 [00:03<01:36,  1.95it/s, loss=4.181, v_num=6664321]Epoch 1:   4%|▎         | 7/196 [00:03<01:36,  1.95it/s, loss=4.035, v_num=6664321]Epoch 1:   4%|▍         | 8/196 [00:03<01:32,  2.03it/s, loss=4.035, v_num=6664321]Epoch 1:   4%|▍         | 8/196 [00:03<01:32,  2.03it/s, loss=3.882, v_num=6664321]Epoch 1:   5%|▍         | 9/196 [00:04<01:29,  2.08it/s, loss=3.882, v_num=6664321]Epoch 1:   5%|▍         | 9/196 [00:04<01:29,  2.08it/s, loss=3.742, v_num=6664321]Epoch 1:   5%|▌         | 10/196 [00:04<01:27,  2.12it/s, loss=3.742, v_num=6664321]Epoch 1:   5%|▌         | 10/196 [00:04<01:27,  2.12it/s, loss=3.609, v_num=6664321]Epoch 1:   6%|▌         | 11/196 [00:05<01:25,  2.17it/s, loss=3.609, v_num=6664321]Epoch 1:   6%|▌         | 11/196 [00:05<01:25,  2.17it/s, loss=3.490, v_num=6664321]Epoch 1:   6%|▌         | 12/196 [00:05<01:23,  2.21it/s, loss=3.490, v_num=6664321]Epoch 1:   6%|▌         | 12/196 [00:05<01:23,  2.21it/s, loss=3.388, v_num=6664321]Epoch 1:   7%|▋         | 13/196 [00:05<01:21,  2.23it/s, loss=3.388, v_num=6664321]Epoch 1:   7%|▋         | 13/196 [00:05<01:21,  2.23it/s, loss=3.306, v_num=6664321]Epoch 1:   7%|▋         | 14/196 [00:06<01:20,  2.26it/s, loss=3.306, v_num=6664321]Epoch 1:   7%|▋         | 14/196 [00:06<01:20,  2.26it/s, loss=3.238, v_num=6664321]Epoch 1:   8%|▊         | 15/196 [00:06<01:19,  2.29it/s, loss=3.238, v_num=6664321]Epoch 1:   8%|▊         | 15/196 [00:06<01:19,  2.29it/s, loss=3.179, v_num=6664321]Epoch 1:   8%|▊         | 16/196 [00:06<01:18,  2.31it/s, loss=3.179, v_num=6664321]Epoch 1:   8%|▊         | 16/196 [00:06<01:18,  2.31it/s, loss=3.125, v_num=6664321]Epoch 1:   9%|▊         | 17/196 [00:07<01:16,  2.33it/s, loss=3.125, v_num=6664321]Epoch 1:   9%|▊         | 17/196 [00:07<01:16,  2.33it/s, loss=3.076, v_num=6664321]Epoch 1:   9%|▉         | 18/196 [00:07<01:15,  2.34it/s, loss=3.076, v_num=6664321]Epoch 1:   9%|▉         | 18/196 [00:07<01:15,  2.34it/s, loss=3.029, v_num=6664321]Epoch 1:  10%|▉         | 19/196 [00:08<01:14,  2.36it/s, loss=3.029, v_num=6664321]Epoch 1:  10%|▉         | 19/196 [00:08<01:14,  2.36it/s, loss=2.988, v_num=6664321]Epoch 1:  10%|█         | 20/196 [00:08<01:14,  2.37it/s, loss=2.988, v_num=6664321]Epoch 1:  10%|█         | 20/196 [00:08<01:14,  2.37it/s, loss=2.950, v_num=6664321]Epoch 1:  11%|█         | 21/196 [00:08<01:13,  2.39it/s, loss=2.950, v_num=6664321]Epoch 1:  11%|█         | 21/196 [00:08<01:13,  2.39it/s, loss=2.940, v_num=6664321]Epoch 1:  11%|█         | 22/196 [00:09<01:12,  2.40it/s, loss=2.940, v_num=6664321]Epoch 1:  11%|█         | 22/196 [00:09<01:12,  2.40it/s, loss=2.745, v_num=6664321]Epoch 1:  12%|█▏        | 23/196 [00:09<01:11,  2.41it/s, loss=2.745, v_num=6664321]Epoch 1:  12%|█▏        | 23/196 [00:09<01:11,  2.41it/s, loss=2.596, v_num=6664321]Epoch 1:  12%|█▏        | 24/196 [00:09<01:10,  2.43it/s, loss=2.596, v_num=6664321]Epoch 1:  12%|█▏        | 24/196 [00:09<01:10,  2.43it/s, loss=2.488, v_num=6664321]Epoch 1:  13%|█▎        | 25/196 [00:10<01:10,  2.43it/s, loss=2.488, v_num=6664321]Epoch 1:  13%|█▎        | 25/196 [00:10<01:10,  2.43it/s, loss=2.403, v_num=6664321]Epoch 1:  13%|█▎        | 26/196 [00:10<01:09,  2.44it/s, loss=2.403, v_num=6664321]Epoch 1:  13%|█▎        | 26/196 [00:10<01:09,  2.44it/s, loss=2.345, v_num=6664321]Epoch 1:  14%|█▍        | 27/196 [00:11<01:08,  2.45it/s, loss=2.345, v_num=6664321]Epoch 1:  14%|█▍        | 27/196 [00:11<01:08,  2.45it/s, loss=2.291, v_num=6664321]Epoch 1:  14%|█▍        | 28/196 [00:11<01:08,  2.46it/s, loss=2.291, v_num=6664321]Epoch 1:  14%|█▍        | 28/196 [00:11<01:08,  2.46it/s, loss=2.254, v_num=6664321]Epoch 1:  15%|█▍        | 29/196 [00:11<01:07,  2.47it/s, loss=2.254, v_num=6664321]Epoch 1:  15%|█▍        | 29/196 [00:11<01:07,  2.47it/s, loss=2.225, v_num=6664321]Epoch 1:  15%|█▌        | 30/196 [00:12<01:07,  2.47it/s, loss=2.225, v_num=6664321]Epoch 1:  15%|█▌        | 30/196 [00:12<01:07,  2.47it/s, loss=2.209, v_num=6664321]Epoch 1:  16%|█▌        | 31/196 [00:12<01:06,  2.48it/s, loss=2.209, v_num=6664321]Epoch 1:  16%|█▌        | 31/196 [00:12<01:06,  2.48it/s, loss=2.199, v_num=6664321]Epoch 1:  16%|█▋        | 32/196 [00:12<01:05,  2.49it/s, loss=2.199, v_num=6664321]Epoch 1:  16%|█▋        | 32/196 [00:12<01:05,  2.49it/s, loss=2.189, v_num=6664321]Epoch 1:  17%|█▋        | 33/196 [00:13<01:05,  2.50it/s, loss=2.189, v_num=6664321]Epoch 1:  17%|█▋        | 33/196 [00:13<01:05,  2.50it/s, loss=2.179, v_num=6664321]Epoch 1:  17%|█▋        | 34/196 [00:13<01:04,  2.50it/s, loss=2.179, v_num=6664321]Epoch 1:  17%|█▋        | 34/196 [00:13<01:04,  2.50it/s, loss=2.165, v_num=6664321]Epoch 1:  18%|█▊        | 35/196 [00:13<01:04,  2.51it/s, loss=2.165, v_num=6664321]Epoch 1:  18%|█▊        | 35/196 [00:13<01:04,  2.51it/s, loss=2.152, v_num=6664321]Epoch 1:  18%|█▊        | 36/196 [00:14<01:03,  2.51it/s, loss=2.152, v_num=6664321]Epoch 1:  18%|█▊        | 36/196 [00:14<01:03,  2.51it/s, loss=2.136, v_num=6664321]Epoch 1:  19%|█▉        | 37/196 [00:14<01:03,  2.52it/s, loss=2.136, v_num=6664321]Epoch 1:  19%|█▉        | 37/196 [00:14<01:03,  2.52it/s, loss=2.123, v_num=6664321]Epoch 1:  19%|█▉        | 38/196 [00:15<01:02,  2.52it/s, loss=2.123, v_num=6664321]Epoch 1:  19%|█▉        | 38/196 [00:15<01:02,  2.52it/s, loss=2.122, v_num=6664321]Epoch 1:  20%|█▉        | 39/196 [00:15<01:02,  2.52it/s, loss=2.122, v_num=6664321]Epoch 1:  20%|█▉        | 39/196 [00:15<01:02,  2.52it/s, loss=2.112, v_num=6664321]Epoch 1:  20%|██        | 40/196 [00:15<01:01,  2.53it/s, loss=2.112, v_num=6664321]Epoch 1:  20%|██        | 40/196 [00:15<01:01,  2.53it/s, loss=2.105, v_num=6664321]Epoch 1:  21%|██        | 41/196 [00:16<01:01,  2.53it/s, loss=2.105, v_num=6664321]Epoch 1:  21%|██        | 41/196 [00:16<01:01,  2.53it/s, loss=2.095, v_num=6664321]Epoch 1:  21%|██▏       | 42/196 [00:16<01:00,  2.54it/s, loss=2.095, v_num=6664321]Epoch 1:  21%|██▏       | 42/196 [00:16<01:00,  2.54it/s, loss=2.089, v_num=6664321]Epoch 1:  22%|██▏       | 43/196 [00:16<01:00,  2.54it/s, loss=2.089, v_num=6664321]Epoch 1:  22%|██▏       | 43/196 [00:16<01:00,  2.54it/s, loss=2.084, v_num=6664321]Epoch 1:  22%|██▏       | 44/196 [00:17<00:59,  2.54it/s, loss=2.084, v_num=6664321]Epoch 1:  22%|██▏       | 44/196 [00:17<00:59,  2.54it/s, loss=2.083, v_num=6664321]Epoch 1:  23%|██▎       | 45/196 [00:17<00:59,  2.55it/s, loss=2.083, v_num=6664321]Epoch 1:  23%|██▎       | 45/196 [00:17<00:59,  2.55it/s, loss=2.077, v_num=6664321]Epoch 1:  23%|██▎       | 46/196 [00:18<00:58,  2.55it/s, loss=2.077, v_num=6664321]Epoch 1:  23%|██▎       | 46/196 [00:18<00:58,  2.55it/s, loss=2.074, v_num=6664321]Epoch 1:  24%|██▍       | 47/196 [00:18<00:58,  2.55it/s, loss=2.074, v_num=6664321]Epoch 1:  24%|██▍       | 47/196 [00:18<00:58,  2.55it/s, loss=2.071, v_num=6664321]Epoch 1:  24%|██▍       | 48/196 [00:18<00:57,  2.55it/s, loss=2.071, v_num=6664321]Epoch 1:  24%|██▍       | 48/196 [00:18<00:57,  2.55it/s, loss=2.067, v_num=6664321]Epoch 1:  25%|██▌       | 49/196 [00:19<00:57,  2.56it/s, loss=2.067, v_num=6664321]Epoch 1:  25%|██▌       | 49/196 [00:19<00:57,  2.56it/s, loss=2.064, v_num=6664321]Epoch 1:  26%|██▌       | 50/196 [00:19<00:57,  2.56it/s, loss=2.064, v_num=6664321]Epoch 1:  26%|██▌       | 50/196 [00:19<00:57,  2.56it/s, loss=2.061, v_num=6664321]Epoch 1:  26%|██▌       | 51/196 [00:19<00:56,  2.56it/s, loss=2.061, v_num=6664321]Epoch 1:  26%|██▌       | 51/196 [00:19<00:56,  2.56it/s, loss=2.058, v_num=6664321]Epoch 1:  27%|██▋       | 52/196 [00:20<00:56,  2.57it/s, loss=2.058, v_num=6664321]Epoch 1:  27%|██▋       | 52/196 [00:20<00:56,  2.57it/s, loss=2.056, v_num=6664321]Epoch 1:  27%|██▋       | 53/196 [00:20<00:55,  2.57it/s, loss=2.056, v_num=6664321]Epoch 1:  27%|██▋       | 53/196 [00:20<00:55,  2.57it/s, loss=2.056, v_num=6664321]Epoch 1:  28%|██▊       | 54/196 [00:21<00:55,  2.57it/s, loss=2.056, v_num=6664321]Epoch 1:  28%|██▊       | 54/196 [00:21<00:55,  2.57it/s, loss=2.059, v_num=6664321]Epoch 1:  28%|██▊       | 55/196 [00:21<00:54,  2.57it/s, loss=2.059, v_num=6664321]Epoch 1:  28%|██▊       | 55/196 [00:21<00:54,  2.57it/s, loss=2.056, v_num=6664321]Epoch 1:  29%|██▊       | 56/196 [00:21<00:54,  2.57it/s, loss=2.056, v_num=6664321]Epoch 1:  29%|██▊       | 56/196 [00:21<00:54,  2.57it/s, loss=2.060, v_num=6664321]Epoch 1:  29%|██▉       | 57/196 [00:22<00:53,  2.58it/s, loss=2.060, v_num=6664321]Epoch 1:  29%|██▉       | 57/196 [00:22<00:53,  2.58it/s, loss=2.057, v_num=6664321]Epoch 1:  30%|██▉       | 58/196 [00:22<00:53,  2.58it/s, loss=2.057, v_num=6664321]Epoch 1:  30%|██▉       | 58/196 [00:22<00:53,  2.58it/s, loss=2.048, v_num=6664321]Epoch 1:  30%|███       | 59/196 [00:22<00:53,  2.58it/s, loss=2.048, v_num=6664321]Epoch 1:  30%|███       | 59/196 [00:22<00:53,  2.58it/s, loss=2.046, v_num=6664321]Epoch 1:  31%|███       | 60/196 [00:23<00:52,  2.58it/s, loss=2.046, v_num=6664321]Epoch 1:  31%|███       | 60/196 [00:23<00:52,  2.58it/s, loss=2.042, v_num=6664321]Epoch 1:  31%|███       | 61/196 [00:23<00:52,  2.58it/s, loss=2.042, v_num=6664321]Epoch 1:  31%|███       | 61/196 [00:23<00:52,  2.58it/s, loss=2.039, v_num=6664321]Epoch 1:  32%|███▏      | 62/196 [00:23<00:51,  2.58it/s, loss=2.039, v_num=6664321]Epoch 1:  32%|███▏      | 62/196 [00:23<00:51,  2.58it/s, loss=2.030, v_num=6664321]Epoch 1:  32%|███▏      | 63/196 [00:24<00:51,  2.59it/s, loss=2.030, v_num=6664321]Epoch 1:  32%|███▏      | 63/196 [00:24<00:51,  2.59it/s, loss=2.028, v_num=6664321]Epoch 1:  33%|███▎      | 64/196 [00:24<00:51,  2.59it/s, loss=2.028, v_num=6664321]Epoch 1:  33%|███▎      | 64/196 [00:24<00:51,  2.59it/s, loss=2.028, v_num=6664321]Epoch 1:  33%|███▎      | 65/196 [00:25<00:50,  2.59it/s, loss=2.028, v_num=6664321]Epoch 1:  33%|███▎      | 65/196 [00:25<00:50,  2.59it/s, loss=2.023, v_num=6664321]Epoch 1:  34%|███▎      | 66/196 [00:25<00:50,  2.59it/s, loss=2.023, v_num=6664321]Epoch 1:  34%|███▎      | 66/196 [00:25<00:50,  2.59it/s, loss=2.017, v_num=6664321]Epoch 1:  34%|███▍      | 67/196 [00:25<00:49,  2.59it/s, loss=2.017, v_num=6664321]Epoch 1:  34%|███▍      | 67/196 [00:25<00:49,  2.59it/s, loss=2.013, v_num=6664321]Epoch 1:  35%|███▍      | 68/196 [00:26<00:49,  2.59it/s, loss=2.013, v_num=6664321]Epoch 1:  35%|███▍      | 68/196 [00:26<00:49,  2.59it/s, loss=2.010, v_num=6664321]Epoch 1:  35%|███▌      | 69/196 [00:26<00:49,  2.59it/s, loss=2.010, v_num=6664321]Epoch 1:  35%|███▌      | 69/196 [00:26<00:49,  2.59it/s, loss=2.008, v_num=6664321]Epoch 1:  36%|███▌      | 70/196 [00:27<00:48,  2.59it/s, loss=2.008, v_num=6664321]Epoch 1:  36%|███▌      | 70/196 [00:27<00:48,  2.59it/s, loss=2.003, v_num=6664321]Epoch 1:  36%|███▌      | 71/196 [00:27<00:48,  2.59it/s, loss=2.003, v_num=6664321]Epoch 1:  36%|███▌      | 71/196 [00:27<00:48,  2.59it/s, loss=1.999, v_num=6664321]Epoch 1:  37%|███▋      | 72/196 [00:27<00:47,  2.59it/s, loss=1.999, v_num=6664321]Epoch 1:  37%|███▋      | 72/196 [00:27<00:47,  2.59it/s, loss=1.991, v_num=6664321]Epoch 1:  37%|███▋      | 73/196 [00:28<00:47,  2.60it/s, loss=1.991, v_num=6664321]Epoch 1:  37%|███▋      | 73/196 [00:28<00:47,  2.60it/s, loss=1.985, v_num=6664321]Epoch 1:  38%|███▊      | 74/196 [00:28<00:47,  2.60it/s, loss=1.985, v_num=6664321]Epoch 1:  38%|███▊      | 74/196 [00:28<00:47,  2.60it/s, loss=1.980, v_num=6664321]Epoch 1:  38%|███▊      | 75/196 [00:28<00:46,  2.60it/s, loss=1.980, v_num=6664321]Epoch 1:  38%|███▊      | 75/196 [00:28<00:46,  2.60it/s, loss=1.978, v_num=6664321]Epoch 1:  39%|███▉      | 76/196 [00:29<00:46,  2.60it/s, loss=1.978, v_num=6664321]Epoch 1:  39%|███▉      | 76/196 [00:29<00:46,  2.60it/s, loss=1.973, v_num=6664321]Epoch 1:  39%|███▉      | 77/196 [00:29<00:45,  2.60it/s, loss=1.973, v_num=6664321]Epoch 1:  39%|███▉      | 77/196 [00:29<00:45,  2.60it/s, loss=1.969, v_num=6664321]Epoch 1:  40%|███▉      | 78/196 [00:29<00:45,  2.60it/s, loss=1.969, v_num=6664321]Epoch 1:  40%|███▉      | 78/196 [00:29<00:45,  2.60it/s, loss=1.967, v_num=6664321]Epoch 1:  40%|████      | 79/196 [00:30<00:44,  2.60it/s, loss=1.967, v_num=6664321]Epoch 1:  40%|████      | 79/196 [00:30<00:44,  2.60it/s, loss=1.966, v_num=6664321]Epoch 1:  41%|████      | 80/196 [00:30<00:44,  2.60it/s, loss=1.966, v_num=6664321]Epoch 1:  41%|████      | 80/196 [00:30<00:44,  2.60it/s, loss=1.960, v_num=6664321]Epoch 1:  41%|████▏     | 81/196 [00:31<00:44,  2.60it/s, loss=1.960, v_num=6664321]Epoch 1:  41%|████▏     | 81/196 [00:31<00:44,  2.60it/s, loss=1.956, v_num=6664321]Epoch 1:  42%|████▏     | 82/196 [00:31<00:43,  2.60it/s, loss=1.956, v_num=6664321]Epoch 1:  42%|████▏     | 82/196 [00:31<00:43,  2.60it/s, loss=1.960, v_num=6664321]Epoch 1:  42%|████▏     | 83/196 [00:31<00:43,  2.60it/s, loss=1.960, v_num=6664321]Epoch 1:  42%|████▏     | 83/196 [00:31<00:43,  2.60it/s, loss=1.962, v_num=6664321]Epoch 1:  43%|████▎     | 84/196 [00:32<00:42,  2.61it/s, loss=1.962, v_num=6664321]Epoch 1:  43%|████▎     | 84/196 [00:32<00:42,  2.60it/s, loss=1.953, v_num=6664321]Epoch 1:  43%|████▎     | 85/196 [00:32<00:42,  2.61it/s, loss=1.953, v_num=6664321]Epoch 1:  43%|████▎     | 85/196 [00:32<00:42,  2.61it/s, loss=1.952, v_num=6664321]Epoch 1:  44%|████▍     | 86/196 [00:32<00:42,  2.61it/s, loss=1.952, v_num=6664321]Epoch 1:  44%|████▍     | 86/196 [00:32<00:42,  2.61it/s, loss=1.954, v_num=6664321]Epoch 1:  44%|████▍     | 87/196 [00:33<00:41,  2.61it/s, loss=1.954, v_num=6664321]Epoch 1:  44%|████▍     | 87/196 [00:33<00:41,  2.61it/s, loss=1.951, v_num=6664321]Epoch 1:  45%|████▍     | 88/196 [00:33<00:41,  2.61it/s, loss=1.951, v_num=6664321]Epoch 1:  45%|████▍     | 88/196 [00:33<00:41,  2.61it/s, loss=1.950, v_num=6664321]Epoch 1:  45%|████▌     | 89/196 [00:34<00:41,  2.61it/s, loss=1.950, v_num=6664321]Epoch 1:  45%|████▌     | 89/196 [00:34<00:41,  2.61it/s, loss=1.945, v_num=6664321]Epoch 1:  46%|████▌     | 90/196 [00:34<00:40,  2.61it/s, loss=1.945, v_num=6664321]Epoch 1:  46%|████▌     | 90/196 [00:34<00:40,  2.61it/s, loss=1.943, v_num=6664321]Epoch 1:  46%|████▋     | 91/196 [00:34<00:40,  2.61it/s, loss=1.943, v_num=6664321]Epoch 1:  46%|████▋     | 91/196 [00:34<00:40,  2.61it/s, loss=1.940, v_num=6664321]Epoch 1:  47%|████▋     | 92/196 [00:35<00:39,  2.61it/s, loss=1.940, v_num=6664321]Epoch 1:  47%|████▋     | 92/196 [00:35<00:39,  2.61it/s, loss=1.938, v_num=6664321]Epoch 1:  47%|████▋     | 93/196 [00:35<00:39,  2.61it/s, loss=1.938, v_num=6664321]Epoch 1:  47%|████▋     | 93/196 [00:35<00:39,  2.61it/s, loss=1.932, v_num=6664321]Epoch 1:  48%|████▊     | 94/196 [00:35<00:39,  2.61it/s, loss=1.932, v_num=6664321]Epoch 1:  48%|████▊     | 94/196 [00:35<00:39,  2.61it/s, loss=1.929, v_num=6664321]Epoch 1:  48%|████▊     | 95/196 [00:36<00:38,  2.61it/s, loss=1.929, v_num=6664321]Epoch 1:  48%|████▊     | 95/196 [00:36<00:38,  2.61it/s, loss=1.924, v_num=6664321]Epoch 1:  49%|████▉     | 96/196 [00:36<00:38,  2.61it/s, loss=1.924, v_num=6664321]Epoch 1:  49%|████▉     | 96/196 [00:36<00:38,  2.61it/s, loss=1.914, v_num=6664321]Epoch 1:  49%|████▉     | 97/196 [00:37<00:37,  2.61it/s, loss=1.914, v_num=6664321]Epoch 1:  49%|████▉     | 97/196 [00:37<00:37,  2.61it/s, loss=1.912, v_num=6664321]Epoch 1:  50%|█████     | 98/196 [00:37<00:37,  2.61it/s, loss=1.912, v_num=6664321]Epoch 1:  50%|█████     | 98/196 [00:37<00:37,  2.61it/s, loss=1.909, v_num=6664321]Epoch 1:  51%|█████     | 99/196 [00:37<00:37,  2.61it/s, loss=1.909, v_num=6664321]Epoch 1:  51%|█████     | 99/196 [00:37<00:37,  2.61it/s, loss=1.905, v_num=6664321]Epoch 1:  51%|█████     | 100/196 [00:38<00:36,  2.61it/s, loss=1.905, v_num=6664321]Epoch 1:  51%|█████     | 100/196 [00:38<00:36,  2.61it/s, loss=1.905, v_num=6664321]Epoch 1:  52%|█████▏    | 101/196 [00:38<00:36,  2.61it/s, loss=1.905, v_num=6664321]Epoch 1:  52%|█████▏    | 101/196 [00:38<00:36,  2.61it/s, loss=1.905, v_num=6664321]Epoch 1:  52%|█████▏    | 102/196 [00:38<00:35,  2.62it/s, loss=1.905, v_num=6664321]Epoch 1:  52%|█████▏    | 102/196 [00:38<00:35,  2.62it/s, loss=1.902, v_num=6664321]Epoch 1:  53%|█████▎    | 103/196 [00:39<00:35,  2.62it/s, loss=1.902, v_num=6664321]Epoch 1:  53%|█████▎    | 103/196 [00:39<00:35,  2.62it/s, loss=1.894, v_num=6664321]Epoch 1:  53%|█████▎    | 104/196 [00:39<00:35,  2.62it/s, loss=1.894, v_num=6664321]Epoch 1:  53%|█████▎    | 104/196 [00:39<00:35,  2.62it/s, loss=1.895, v_num=6664321]Epoch 1:  54%|█████▎    | 105/196 [00:40<00:34,  2.62it/s, loss=1.895, v_num=6664321]Epoch 1:  54%|█████▎    | 105/196 [00:40<00:34,  2.62it/s, loss=1.889, v_num=6664321]Epoch 1:  54%|█████▍    | 106/196 [00:40<00:34,  2.62it/s, loss=1.889, v_num=6664321]Epoch 1:  54%|█████▍    | 106/196 [00:40<00:34,  2.62it/s, loss=1.880, v_num=6664321]Epoch 1:  55%|█████▍    | 107/196 [00:40<00:34,  2.62it/s, loss=1.880, v_num=6664321]Epoch 1:  55%|█████▍    | 107/196 [00:40<00:34,  2.62it/s, loss=1.884, v_num=6664321]Epoch 1:  55%|█████▌    | 108/196 [00:41<00:33,  2.62it/s, loss=1.884, v_num=6664321]Epoch 1:  55%|█████▌    | 108/196 [00:41<00:33,  2.62it/s, loss=1.886, v_num=6664321]Epoch 1:  56%|█████▌    | 109/196 [00:41<00:33,  2.62it/s, loss=1.886, v_num=6664321]Epoch 1:  56%|█████▌    | 109/196 [00:41<00:33,  2.62it/s, loss=1.885, v_num=6664321]Epoch 1:  56%|█████▌    | 110/196 [00:42<00:32,  2.62it/s, loss=1.885, v_num=6664321]Epoch 1:  56%|█████▌    | 110/196 [00:42<00:32,  2.62it/s, loss=1.883, v_num=6664321]Epoch 1:  57%|█████▋    | 111/196 [00:42<00:32,  2.62it/s, loss=1.883, v_num=6664321]Epoch 1:  57%|█████▋    | 111/196 [00:42<00:32,  2.62it/s, loss=1.884, v_num=6664321]Epoch 1:  57%|█████▋    | 112/196 [00:42<00:32,  2.62it/s, loss=1.884, v_num=6664321]Epoch 1:  57%|█████▋    | 112/196 [00:42<00:32,  2.62it/s, loss=1.885, v_num=6664321]Epoch 1:  58%|█████▊    | 113/196 [00:43<00:31,  2.62it/s, loss=1.885, v_num=6664321]Epoch 1:  58%|█████▊    | 113/196 [00:43<00:31,  2.62it/s, loss=1.886, v_num=6664321]Epoch 1:  58%|█████▊    | 114/196 [00:43<00:31,  2.62it/s, loss=1.886, v_num=6664321]Epoch 1:  58%|█████▊    | 114/196 [00:43<00:31,  2.62it/s, loss=1.879, v_num=6664321]Epoch 1:  59%|█████▊    | 115/196 [00:43<00:30,  2.62it/s, loss=1.879, v_num=6664321]Epoch 1:  59%|█████▊    | 115/196 [00:43<00:30,  2.62it/s, loss=1.877, v_num=6664321]Epoch 1:  59%|█████▉    | 116/196 [00:44<00:30,  2.62it/s, loss=1.877, v_num=6664321]Epoch 1:  59%|█████▉    | 116/196 [00:44<00:30,  2.62it/s, loss=1.879, v_num=6664321]Epoch 1:  60%|█████▉    | 117/196 [00:44<00:30,  2.62it/s, loss=1.879, v_num=6664321]Epoch 1:  60%|█████▉    | 117/196 [00:44<00:30,  2.62it/s, loss=1.877, v_num=6664321]Epoch 1:  60%|██████    | 118/196 [00:45<00:29,  2.62it/s, loss=1.877, v_num=6664321]Epoch 1:  60%|██████    | 118/196 [00:45<00:29,  2.62it/s, loss=1.875, v_num=6664321]Epoch 1:  61%|██████    | 119/196 [00:45<00:29,  2.62it/s, loss=1.875, v_num=6664321]Epoch 1:  61%|██████    | 119/196 [00:45<00:29,  2.62it/s, loss=1.869, v_num=6664321]Epoch 1:  61%|██████    | 120/196 [00:45<00:28,  2.62it/s, loss=1.869, v_num=6664321]Epoch 1:  61%|██████    | 120/196 [00:45<00:28,  2.62it/s, loss=1.865, v_num=6664321]Epoch 1:  62%|██████▏   | 121/196 [00:46<00:28,  2.62it/s, loss=1.865, v_num=6664321]Epoch 1:  62%|██████▏   | 121/196 [00:46<00:28,  2.62it/s, loss=1.862, v_num=6664321]Epoch 1:  62%|██████▏   | 122/196 [00:46<00:28,  2.62it/s, loss=1.862, v_num=6664321]Epoch 1:  62%|██████▏   | 122/196 [00:46<00:28,  2.62it/s, loss=1.853, v_num=6664321]Epoch 1:  63%|██████▎   | 123/196 [00:46<00:27,  2.62it/s, loss=1.853, v_num=6664321]Epoch 1:  63%|██████▎   | 123/196 [00:46<00:27,  2.62it/s, loss=1.848, v_num=6664321]Epoch 1:  63%|██████▎   | 124/196 [00:47<00:27,  2.63it/s, loss=1.848, v_num=6664321]Epoch 1:  63%|██████▎   | 124/196 [00:47<00:27,  2.63it/s, loss=1.841, v_num=6664321]Epoch 1:  64%|██████▍   | 125/196 [00:47<00:27,  2.63it/s, loss=1.841, v_num=6664321]Epoch 1:  64%|██████▍   | 125/196 [00:47<00:27,  2.63it/s, loss=1.841, v_num=6664321]Epoch 1:  64%|██████▍   | 126/196 [00:47<00:26,  2.63it/s, loss=1.841, v_num=6664321]Epoch 1:  64%|██████▍   | 126/196 [00:47<00:26,  2.63it/s, loss=1.841, v_num=6664321]Epoch 1:  65%|██████▍   | 127/196 [00:48<00:26,  2.63it/s, loss=1.841, v_num=6664321]Epoch 1:  65%|██████▍   | 127/196 [00:48<00:26,  2.63it/s, loss=1.832, v_num=6664321]Epoch 1:  65%|██████▌   | 128/196 [00:48<00:25,  2.63it/s, loss=1.832, v_num=6664321]Epoch 1:  65%|██████▌   | 128/196 [00:48<00:25,  2.63it/s, loss=1.828, v_num=6664321]Epoch 1:  66%|██████▌   | 129/196 [00:49<00:25,  2.63it/s, loss=1.828, v_num=6664321]Epoch 1:  66%|██████▌   | 129/196 [00:49<00:25,  2.63it/s, loss=1.823, v_num=6664321]Epoch 1:  66%|██████▋   | 130/196 [00:49<00:25,  2.63it/s, loss=1.823, v_num=6664321]Epoch 1:  66%|██████▋   | 130/196 [00:49<00:25,  2.63it/s, loss=1.823, v_num=6664321]Epoch 1:  67%|██████▋   | 131/196 [00:49<00:24,  2.63it/s, loss=1.823, v_num=6664321]Epoch 1:  67%|██████▋   | 131/196 [00:49<00:24,  2.63it/s, loss=1.815, v_num=6664321]Epoch 1:  67%|██████▋   | 132/196 [00:50<00:24,  2.63it/s, loss=1.815, v_num=6664321]Epoch 1:  67%|██████▋   | 132/196 [00:50<00:24,  2.63it/s, loss=1.810, v_num=6664321]Epoch 1:  68%|██████▊   | 133/196 [00:50<00:23,  2.63it/s, loss=1.810, v_num=6664321]Epoch 1:  68%|██████▊   | 133/196 [00:50<00:23,  2.63it/s, loss=1.805, v_num=6664321]Epoch 1:  68%|██████▊   | 134/196 [00:50<00:23,  2.63it/s, loss=1.805, v_num=6664321]Epoch 1:  68%|██████▊   | 134/196 [00:50<00:23,  2.63it/s, loss=1.804, v_num=6664321]Epoch 1:  69%|██████▉   | 135/196 [00:51<00:23,  2.63it/s, loss=1.804, v_num=6664321]Epoch 1:  69%|██████▉   | 135/196 [00:51<00:23,  2.63it/s, loss=1.800, v_num=6664321]Epoch 1:  69%|██████▉   | 136/196 [00:51<00:22,  2.63it/s, loss=1.800, v_num=6664321]Epoch 1:  69%|██████▉   | 136/196 [00:51<00:22,  2.63it/s, loss=1.798, v_num=6664321]Epoch 1:  70%|██████▉   | 137/196 [00:52<00:22,  2.63it/s, loss=1.798, v_num=6664321]Epoch 1:  70%|██████▉   | 137/196 [00:52<00:22,  2.63it/s, loss=1.792, v_num=6664321]Epoch 1:  70%|███████   | 138/196 [00:52<00:22,  2.63it/s, loss=1.792, v_num=6664321]Epoch 1:  70%|███████   | 138/196 [00:52<00:22,  2.63it/s, loss=1.790, v_num=6664321]Epoch 1:  71%|███████   | 139/196 [00:52<00:21,  2.63it/s, loss=1.790, v_num=6664321]Epoch 1:  71%|███████   | 139/196 [00:52<00:21,  2.63it/s, loss=1.793, v_num=6664321]Epoch 1:  71%|███████▏  | 140/196 [00:53<00:21,  2.63it/s, loss=1.793, v_num=6664321]Epoch 1:  71%|███████▏  | 140/196 [00:53<00:21,  2.63it/s, loss=1.791, v_num=6664321]Epoch 1:  72%|███████▏  | 141/196 [00:53<00:20,  2.63it/s, loss=1.791, v_num=6664321]Epoch 1:  72%|███████▏  | 141/196 [00:53<00:20,  2.63it/s, loss=1.785, v_num=6664321]Epoch 1:  72%|███████▏  | 142/196 [00:53<00:20,  2.63it/s, loss=1.785, v_num=6664321]Epoch 1:  72%|███████▏  | 142/196 [00:53<00:20,  2.63it/s, loss=1.789, v_num=6664321]Epoch 1:  73%|███████▎  | 143/196 [00:54<00:20,  2.63it/s, loss=1.789, v_num=6664321]Epoch 1:  73%|███████▎  | 143/196 [00:54<00:20,  2.63it/s, loss=1.787, v_num=6664321]Epoch 1:  73%|███████▎  | 144/196 [00:54<00:19,  2.63it/s, loss=1.787, v_num=6664321]Epoch 1:  73%|███████▎  | 144/196 [00:54<00:19,  2.63it/s, loss=1.784, v_num=6664321]Epoch 1:  74%|███████▍  | 145/196 [00:55<00:19,  2.63it/s, loss=1.784, v_num=6664321]Epoch 1:  74%|███████▍  | 145/196 [00:55<00:19,  2.63it/s, loss=1.781, v_num=6664321]Epoch 1:  74%|███████▍  | 146/196 [00:55<00:19,  2.63it/s, loss=1.781, v_num=6664321]Epoch 1:  74%|███████▍  | 146/196 [00:55<00:19,  2.63it/s, loss=1.779, v_num=6664321]Epoch 1:  75%|███████▌  | 147/196 [00:55<00:18,  2.63it/s, loss=1.779, v_num=6664321]Epoch 1:  75%|███████▌  | 147/196 [00:55<00:18,  2.63it/s, loss=1.775, v_num=6664321]Epoch 1:  76%|███████▌  | 148/196 [00:56<00:18,  2.63it/s, loss=1.775, v_num=6664321]Epoch 1:  76%|███████▌  | 148/196 [00:56<00:18,  2.63it/s, loss=1.769, v_num=6664321]Epoch 1:  76%|███████▌  | 149/196 [00:56<00:17,  2.63it/s, loss=1.769, v_num=6664321]Epoch 1:  76%|███████▌  | 149/196 [00:56<00:17,  2.63it/s, loss=1.769, v_num=6664321]Epoch 1:  77%|███████▋  | 150/196 [00:56<00:17,  2.63it/s, loss=1.769, v_num=6664321]Epoch 1:  77%|███████▋  | 150/196 [00:56<00:17,  2.63it/s, loss=1.766, v_num=6664321]Epoch 1:  77%|███████▋  | 151/196 [00:57<00:17,  2.63it/s, loss=1.766, v_num=6664321]Epoch 1:  77%|███████▋  | 151/196 [00:57<00:17,  2.63it/s, loss=1.762, v_num=6664321]Epoch 1:  78%|███████▊  | 152/196 [00:57<00:16,  2.63it/s, loss=1.762, v_num=6664321]Epoch 1:  78%|███████▊  | 152/196 [00:57<00:16,  2.63it/s, loss=1.753, v_num=6664321]Epoch 1:  78%|███████▊  | 153/196 [00:58<00:16,  2.63it/s, loss=1.753, v_num=6664321]Epoch 1:  78%|███████▊  | 153/196 [00:58<00:16,  2.63it/s, loss=1.751, v_num=6664321]slurmstepd: error: *** JOB 6664321 ON gr042-ib0 CANCELLED AT 2021-05-10T12:29:33 ***
