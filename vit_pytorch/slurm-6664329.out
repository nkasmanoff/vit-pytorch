/scratch/nsk367/anaconda3/envs/vit/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO:lightning:GPU available: True, used: True
INFO:lightning:CUDA_VISIBLE_DEVICES: [0]
INFO:lightning:Set SLURM handle signals.
INFO:lightning:
    | Name                                                    | Type        | Params
------------------------------------------------------------------------------------
0   | _ViT_Trainer__model                                     | ViT         | 25 M  
1   | _ViT_Trainer__model.to_patch_embedding                  | Sequential  | 25 K  
2   | _ViT_Trainer__model.to_patch_embedding.0                | Rearrange   | 0     
3   | _ViT_Trainer__model.to_patch_embedding.1                | Linear      | 25 K  
4   | _ViT_Trainer__model.dropout                             | Dropout     | 0     
5   | _ViT_Trainer__model.transformer                         | Transformer | 25 M  
6   | _ViT_Trainer__model.transformer.layers                  | ModuleList  | 25 M  
7   | _ViT_Trainer__model.transformer.layers.0                | ModuleList  | 2 M   
8   | _ViT_Trainer__model.transformer.layers.0.0              | PreNorm     | 1 M   
9   | _ViT_Trainer__model.transformer.layers.0.0.norm         | LayerNorm   | 1 K   
10  | _ViT_Trainer__model.transformer.layers.0.0.fn           | Attention   | 1 M   
11  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_qkv    | Linear      | 1 M   
12  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out    | Sequential  | 393 K 
13  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out.0  | Linear      | 393 K 
14  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out.1  | Dropout     | 0     
15  | _ViT_Trainer__model.transformer.layers.0.1              | PreNorm     | 526 K 
16  | _ViT_Trainer__model.transformer.layers.0.1.norm         | LayerNorm   | 1 K   
17  | _ViT_Trainer__model.transformer.layers.0.1.fn           | FeedForward | 525 K 
18  | _ViT_Trainer__model.transformer.layers.0.1.fn.net       | Sequential  | 525 K 
19  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.0     | Linear      | 262 K 
20  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.1     | GELU        | 0     
21  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.2     | Dropout     | 0     
22  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.3     | Linear      | 262 K 
23  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.4     | Dropout     | 0     
24  | _ViT_Trainer__model.transformer.layers.1                | ModuleList  | 2 M   
25  | _ViT_Trainer__model.transformer.layers.1.0              | PreNorm     | 1 M   
26  | _ViT_Trainer__model.transformer.layers.1.0.norm         | LayerNorm   | 1 K   
27  | _ViT_Trainer__model.transformer.layers.1.0.fn           | Attention   | 1 M   
28  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_qkv    | Linear      | 1 M   
29  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out    | Sequential  | 393 K 
30  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out.0  | Linear      | 393 K 
31  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out.1  | Dropout     | 0     
32  | _ViT_Trainer__model.transformer.layers.1.1              | PreNorm     | 526 K 
33  | _ViT_Trainer__model.transformer.layers.1.1.norm         | LayerNorm   | 1 K   
34  | _ViT_Trainer__model.transformer.layers.1.1.fn           | FeedForward | 525 K 
35  | _ViT_Trainer__model.transformer.layers.1.1.fn.net       | Sequential  | 525 K 
36  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.0     | Linear      | 262 K 
37  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.1     | GELU        | 0     
38  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.2     | Dropout     | 0     
39  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.3     | Linear      | 262 K 
40  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.4     | Dropout     | 0     
41  | _ViT_Trainer__model.transformer.layers.2                | ModuleList  | 2 M   
42  | _ViT_Trainer__model.transformer.layers.2.0              | PreNorm     | 1 M   
43  | _ViT_Trainer__model.transformer.layers.2.0.norm         | LayerNorm   | 1 K   
44  | _ViT_Trainer__model.transformer.layers.2.0.fn           | Attention   | 1 M   
45  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_qkv    | Linear      | 1 M   
46  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out    | Sequential  | 393 K 
47  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out.0  | Linear      | 393 K 
48  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out.1  | Dropout     | 0     
49  | _ViT_Trainer__model.transformer.layers.2.1              | PreNorm     | 526 K 
50  | _ViT_Trainer__model.transformer.layers.2.1.norm         | LayerNorm   | 1 K   
51  | _ViT_Trainer__model.transformer.layers.2.1.fn           | FeedForward | 525 K 
52  | _ViT_Trainer__model.transformer.layers.2.1.fn.net       | Sequential  | 525 K 
53  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.0     | Linear      | 262 K 
54  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.1     | GELU        | 0     
55  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.2     | Dropout     | 0     
56  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.3     | Linear      | 262 K 
57  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.4     | Dropout     | 0     
58  | _ViT_Trainer__model.transformer.layers.3                | ModuleList  | 2 M   
59  | _ViT_Trainer__model.transformer.layers.3.0              | PreNorm     | 1 M   
60  | _ViT_Trainer__model.transformer.layers.3.0.norm         | LayerNorm   | 1 K   
61  | _ViT_Trainer__model.transformer.layers.3.0.fn           | Attention   | 1 M   
62  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_qkv    | Linear      | 1 M   
63  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out    | Sequential  | 393 K 
64  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out.0  | Linear      | 393 K 
65  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out.1  | Dropout     | 0     
66  | _ViT_Trainer__model.transformer.layers.3.1              | PreNorm     | 526 K 
67  | _ViT_Trainer__model.transformer.layers.3.1.norm         | LayerNorm   | 1 K   
68  | _ViT_Trainer__model.transformer.layers.3.1.fn           | FeedForward | 525 K 
69  | _ViT_Trainer__model.transformer.layers.3.1.fn.net       | Sequential  | 525 K 
70  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.0     | Linear      | 262 K 
71  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.1     | GELU        | 0     
72  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.2     | Dropout     | 0     
73  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.3     | Linear      | 262 K 
74  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.4     | Dropout     | 0     
75  | _ViT_Trainer__model.transformer.layers.4                | ModuleList  | 2 M   
76  | _ViT_Trainer__model.transformer.layers.4.0              | PreNorm     | 1 M   
77  | _ViT_Trainer__model.transformer.layers.4.0.norm         | LayerNorm   | 1 K   
78  | _ViT_Trainer__model.transformer.layers.4.0.fn           | Attention   | 1 M   
79  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_qkv    | Linear      | 1 M   
80  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out    | Sequential  | 393 K 
81  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out.0  | Linear      | 393 K 
82  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out.1  | Dropout     | 0     
83  | _ViT_Trainer__model.transformer.layers.4.1              | PreNorm     | 526 K 
84  | _ViT_Trainer__model.transformer.layers.4.1.norm         | LayerNorm   | 1 K   
85  | _ViT_Trainer__model.transformer.layers.4.1.fn           | FeedForward | 525 K 
86  | _ViT_Trainer__model.transformer.layers.4.1.fn.net       | Sequential  | 525 K 
87  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.0     | Linear      | 262 K 
88  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.1     | GELU        | 0     
89  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.2     | Dropout     | 0     
90  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.3     | Linear      | 262 K 
91  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.4     | Dropout     | 0     
92  | _ViT_Trainer__model.transformer.layers.5                | ModuleList  | 2 M   
93  | _ViT_Trainer__model.transformer.layers.5.0              | PreNorm     | 1 M   
94  | _ViT_Trainer__model.transformer.layers.5.0.norm         | LayerNorm   | 1 K   
95  | _ViT_Trainer__model.transformer.layers.5.0.fn           | Attention   | 1 M   
96  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_qkv    | Linear      | 1 M   
97  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out    | Sequential  | 393 K 
98  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out.0  | Linear      | 393 K 
99  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out.1  | Dropout     | 0     
100 | _ViT_Trainer__model.transformer.layers.5.1              | PreNorm     | 526 K 
101 | _ViT_Trainer__model.transformer.layers.5.1.norm         | LayerNorm   | 1 K   
102 | _ViT_Trainer__model.transformer.layers.5.1.fn           | FeedForward | 525 K 
103 | _ViT_Trainer__model.transformer.layers.5.1.fn.net       | Sequential  | 525 K 
104 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.0     | Linear      | 262 K 
105 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.1     | GELU        | 0     
106 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.2     | Dropout     | 0     
107 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.3     | Linear      | 262 K 
108 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.4     | Dropout     | 0     
109 | _ViT_Trainer__model.transformer.layers.6                | ModuleList  | 2 M   
110 | _ViT_Trainer__model.transformer.layers.6.0              | PreNorm     | 1 M   
111 | _ViT_Trainer__model.transformer.layers.6.0.norm         | LayerNorm   | 1 K   
112 | _ViT_Trainer__model.transformer.layers.6.0.fn           | Attention   | 1 M   
113 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_qkv    | Linear      | 1 M   
114 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out    | Sequential  | 393 K 
115 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out.0  | Linear      | 393 K 
116 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out.1  | Dropout     | 0     
117 | _ViT_Trainer__model.transformer.layers.6.1              | PreNorm     | 526 K 
118 | _ViT_Trainer__model.transformer.layers.6.1.norm         | LayerNorm   | 1 K   
119 | _ViT_Trainer__model.transformer.layers.6.1.fn           | FeedForward | 525 K 
120 | _ViT_Trainer__model.transformer.layers.6.1.fn.net       | Sequential  | 525 K 
121 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.0     | Linear      | 262 K 
122 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.1     | GELU        | 0     
123 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.2     | Dropout     | 0     
124 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.3     | Linear      | 262 K 
125 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.4     | Dropout     | 0     
126 | _ViT_Trainer__model.transformer.layers.7                | ModuleList  | 2 M   
127 | _ViT_Trainer__model.transformer.layers.7.0              | PreNorm     | 1 M   
128 | _ViT_Trainer__model.transformer.layers.7.0.norm         | LayerNorm   | 1 K   
129 | _ViT_Trainer__model.transformer.layers.7.0.fn           | Attention   | 1 M   
130 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_qkv    | Linear      | 1 M   
131 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out    | Sequential  | 393 K 
132 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out.0  | Linear      | 393 K 
133 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out.1  | Dropout     | 0     
134 | _ViT_Trainer__model.transformer.layers.7.1              | PreNorm     | 526 K 
135 | _ViT_Trainer__model.transformer.layers.7.1.norm         | LayerNorm   | 1 K   
136 | _ViT_Trainer__model.transformer.layers.7.1.fn           | FeedForward | 525 K 
137 | _ViT_Trainer__model.transformer.layers.7.1.fn.net       | Sequential  | 525 K 
138 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.0     | Linear      | 262 K 
139 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.1     | GELU        | 0     
140 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.2     | Dropout     | 0     
141 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.3     | Linear      | 262 K 
142 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.4     | Dropout     | 0     
143 | _ViT_Trainer__model.transformer.layers.8                | ModuleList  | 2 M   
144 | _ViT_Trainer__model.transformer.layers.8.0              | PreNorm     | 1 M   
145 | _ViT_Trainer__model.transformer.layers.8.0.norm         | LayerNorm   | 1 K   
146 | _ViT_Trainer__model.transformer.layers.8.0.fn           | Attention   | 1 M   
147 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_qkv    | Linear      | 1 M   
148 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_out    | Sequential  | 393 K 
149 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_out.0  | Linear      | 393 K 
150 | _ViT_Trainer__model.transformer.layers.8.0.fn.to_out.1  | Dropout     | 0     
151 | _ViT_Trainer__model.transformer.layers.8.1              | PreNorm     | 526 K 
152 | _ViT_Trainer__model.transformer.layers.8.1.norm         | LayerNorm   | 1 K   
153 | _ViT_Trainer__model.transformer.layers.8.1.fn           | FeedForward | 525 K 
154 | _ViT_Trainer__model.transformer.layers.8.1.fn.net       | Sequential  | 525 K 
155 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.0     | Linear      | 262 K 
156 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.1     | GELU        | 0     
157 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.2     | Dropout     | 0     
158 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.3     | Linear      | 262 K 
159 | _ViT_Trainer__model.transformer.layers.8.1.fn.net.4     | Dropout     | 0     
160 | _ViT_Trainer__model.transformer.layers.9                | ModuleList  | 2 M   
161 | _ViT_Trainer__model.transformer.layers.9.0              | PreNorm     | 1 M   
162 | _ViT_Trainer__model.transformer.layers.9.0.norm         | LayerNorm   | 1 K   
163 | _ViT_Trainer__model.transformer.layers.9.0.fn           | Attention   | 1 M   
164 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_qkv    | Linear      | 1 M   
165 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_out    | Sequential  | 393 K 
166 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_out.0  | Linear      | 393 K 
167 | _ViT_Trainer__model.transformer.layers.9.0.fn.to_out.1  | Dropout     | 0     
168 | _ViT_Trainer__model.transformer.layers.9.1              | PreNorm     | 526 K 
169 | _ViT_Trainer__model.transformer.layers.9.1.norm         | LayerNorm   | 1 K   
170 | _ViT_Trainer__model.transformer.layers.9.1.fn           | FeedForward | 525 K 
171 | _ViT_Trainer__model.transformer.layers.9.1.fn.net       | Sequential  | 525 K 
172 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.0     | Linear      | 262 K 
173 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.1     | GELU        | 0     
174 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.2     | Dropout     | 0     
175 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.3     | Linear      | 262 K 
176 | _ViT_Trainer__model.transformer.layers.9.1.fn.net.4     | Dropout     | 0     
177 | _ViT_Trainer__model.transformer.layers.10               | ModuleList  | 2 M   
178 | _ViT_Trainer__model.transformer.layers.10.0             | PreNorm     | 1 M   
179 | _ViT_Trainer__model.transformer.layers.10.0.norm        | LayerNorm   | 1 K   
180 | _ViT_Trainer__model.transformer.layers.10.0.fn          | Attention   | 1 M   
181 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_qkv   | Linear      | 1 M   
182 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_out   | Sequential  | 393 K 
183 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_out.0 | Linear      | 393 K 
184 | _ViT_Trainer__model.transformer.layers.10.0.fn.to_out.1 | Dropout     | 0     
185 | _ViT_Trainer__model.transformer.layers.10.1             | PreNorm     | 526 K 
186 | _ViT_Trainer__model.transformer.layers.10.1.norm        | LayerNorm   | 1 K   
187 | _ViT_Trainer__model.transformer.layers.10.1.fn          | FeedForward | 525 K 
188 | _ViT_Trainer__model.transformer.layers.10.1.fn.net      | Sequential  | 525 K 
189 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.0    | Linear      | 262 K 
190 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.1    | GELU        | 0     
191 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.2    | Dropout     | 0     
192 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.3    | Linear      | 262 K 
193 | _ViT_Trainer__model.transformer.layers.10.1.fn.net.4    | Dropout     | 0     
194 | _ViT_Trainer__model.transformer.layers.11               | ModuleList  | 2 M   
195 | _ViT_Trainer__model.transformer.layers.11.0             | PreNorm     | 1 M   
196 | _ViT_Trainer__model.transformer.layers.11.0.norm        | LayerNorm   | 1 K   
197 | _ViT_Trainer__model.transformer.layers.11.0.fn          | Attention   | 1 M   
198 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_qkv   | Linear      | 1 M   
199 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_out   | Sequential  | 393 K 
200 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_out.0 | Linear      | 393 K 
201 | _ViT_Trainer__model.transformer.layers.11.0.fn.to_out.1 | Dropout     | 0     
202 | _ViT_Trainer__model.transformer.layers.11.1             | PreNorm     | 526 K 
203 | _ViT_Trainer__model.transformer.layers.11.1.norm        | LayerNorm   | 1 K   
204 | _ViT_Trainer__model.transformer.layers.11.1.fn          | FeedForward | 525 K 
205 | _ViT_Trainer__model.transformer.layers.11.1.fn.net      | Sequential  | 525 K 
206 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.0    | Linear      | 262 K 
207 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.1    | GELU        | 0     
208 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.2    | Dropout     | 0     
209 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.3    | Linear      | 262 K 
210 | _ViT_Trainer__model.transformer.layers.11.1.fn.net.4    | Dropout     | 0     
211 | _ViT_Trainer__model.to_latent                           | Identity    | 0     
212 | _ViT_Trainer__model.mlp_head                            | Sequential  | 6 K   
213 | _ViT_Trainer__model.mlp_head.0                          | LayerNorm   | 1 K   
214 | _ViT_Trainer__model.mlp_head.1                          | Linear      | 5 K   
Files already downloaded and verified
Files already downloaded and verified
Validation sanity check: 0it [00:00, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Validation sanity check:  20%|██        | 1/5 [00:04<00:17,  4.26s/it]Validation sanity check:  40%|████      | 2/5 [00:04<00:05,  1.85s/it]Validation sanity check:  60%|██████    | 3/5 [00:04<00:02,  1.06s/it]Validation sanity check:  80%|████████  | 4/5 [00:04<00:00,  1.44it/s]Validation sanity check: 100%|██████████| 5/5 [00:04<00:00,  2.05it/s]                                                                      Training: 0it [00:00, ?it/s]Training:   0%|          | 0/196 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s] /scratch/nsk367/anaconda3/envs/vit/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 1:   1%|          | 1/196 [00:01<04:21,  1.34s/it]Epoch 1:   1%|          | 1/196 [00:01<04:22,  1.34s/it, loss=2.397, v_num=6664329]Epoch 1:   1%|          | 2/196 [00:01<02:46,  1.17it/s, loss=2.397, v_num=6664329]Epoch 1:   1%|          | 2/196 [00:01<02:46,  1.17it/s, loss=3.827, v_num=6664329]Epoch 1:   2%|▏         | 3/196 [00:02<02:13,  1.44it/s, loss=3.827, v_num=6664329]Epoch 1:   2%|▏         | 3/196 [00:02<02:13,  1.44it/s, loss=3.805, v_num=6664329]Epoch 1:   2%|▏         | 4/196 [00:02<01:57,  1.63it/s, loss=3.805, v_num=6664329]Epoch 1:   2%|▏         | 4/196 [00:02<01:57,  1.63it/s, loss=3.720, v_num=6664329]Epoch 1:   3%|▎         | 5/196 [00:02<01:47,  1.77it/s, loss=3.720, v_num=6664329]Epoch 1:   3%|▎         | 5/196 [00:02<01:47,  1.77it/s, loss=3.609, v_num=6664329]Epoch 1:   3%|▎         | 6/196 [00:03<01:41,  1.88it/s, loss=3.609, v_num=6664329]Epoch 1:   3%|▎         | 6/196 [00:03<01:41,  1.88it/s, loss=3.499, v_num=6664329]Epoch 1:   4%|▎         | 7/196 [00:03<01:36,  1.96it/s, loss=3.499, v_num=6664329]Epoch 1:   4%|▎         | 7/196 [00:03<01:36,  1.96it/s, loss=3.383, v_num=6664329]Epoch 1:   4%|▍         | 8/196 [00:03<01:32,  2.03it/s, loss=3.383, v_num=6664329]Epoch 1:   4%|▍         | 8/196 [00:03<01:32,  2.03it/s, loss=3.281, v_num=6664329]Epoch 1:   5%|▍         | 9/196 [00:04<01:29,  2.09it/s, loss=3.281, v_num=6664329]Epoch 1:   5%|▍         | 9/196 [00:04<01:29,  2.09it/s, loss=3.172, v_num=6664329]Epoch 1:   5%|▌         | 10/196 [00:04<01:27,  2.14it/s, loss=3.172, v_num=6664329]Epoch 1:   5%|▌         | 10/196 [00:04<01:27,  2.14it/s, loss=3.078, v_num=6664329]Epoch 1:   6%|▌         | 11/196 [00:05<01:25,  2.17it/s, loss=3.078, v_num=6664329]Epoch 1:   6%|▌         | 11/196 [00:05<01:25,  2.17it/s, loss=3.003, v_num=6664329]Epoch 1:   6%|▌         | 12/196 [00:05<01:23,  2.21it/s, loss=3.003, v_num=6664329]Epoch 1:   6%|▌         | 12/196 [00:05<01:23,  2.21it/s, loss=2.950, v_num=6664329]Epoch 1:   7%|▋         | 13/196 [00:05<01:21,  2.24it/s, loss=2.950, v_num=6664329]Epoch 1:   7%|▋         | 13/196 [00:05<01:21,  2.24it/s, loss=2.893, v_num=6664329]Epoch 1:   7%|▋         | 14/196 [00:06<01:20,  2.27it/s, loss=2.893, v_num=6664329]Epoch 1:   7%|▋         | 14/196 [00:06<01:20,  2.27it/s, loss=2.850, v_num=6664329]Epoch 1:   8%|▊         | 15/196 [00:06<01:18,  2.29it/s, loss=2.850, v_num=6664329]Epoch 1:   8%|▊         | 15/196 [00:06<01:18,  2.29it/s, loss=2.809, v_num=6664329]Epoch 1:   8%|▊         | 16/196 [00:06<01:17,  2.31it/s, loss=2.809, v_num=6664329]Epoch 1:   8%|▊         | 16/196 [00:06<01:17,  2.31it/s, loss=2.774, v_num=6664329]Epoch 1:   9%|▊         | 17/196 [00:07<01:16,  2.33it/s, loss=2.774, v_num=6664329]Epoch 1:   9%|▊         | 17/196 [00:07<01:16,  2.33it/s, loss=2.738, v_num=6664329]Epoch 1:   9%|▉         | 18/196 [00:07<01:15,  2.35it/s, loss=2.738, v_num=6664329]Epoch 1:   9%|▉         | 18/196 [00:07<01:15,  2.35it/s, loss=2.710, v_num=6664329]Epoch 1:  10%|▉         | 19/196 [00:08<01:14,  2.37it/s, loss=2.710, v_num=6664329]Epoch 1:  10%|▉         | 19/196 [00:08<01:14,  2.37it/s, loss=2.685, v_num=6664329]Epoch 1:  10%|█         | 20/196 [00:08<01:14,  2.38it/s, loss=2.685, v_num=6664329]Epoch 1:  10%|█         | 20/196 [00:08<01:14,  2.38it/s, loss=2.664, v_num=6664329]Epoch 1:  11%|█         | 21/196 [00:08<01:13,  2.39it/s, loss=2.664, v_num=6664329]Epoch 1:  11%|█         | 21/196 [00:08<01:13,  2.39it/s, loss=2.655, v_num=6664329]Epoch 1:  11%|█         | 22/196 [00:09<01:12,  2.41it/s, loss=2.655, v_num=6664329]Epoch 1:  11%|█         | 22/196 [00:09<01:12,  2.41it/s, loss=2.502, v_num=6664329]Epoch 1:  12%|█▏        | 23/196 [00:09<01:11,  2.42it/s, loss=2.502, v_num=6664329]Epoch 1:  12%|█▏        | 23/196 [00:09<01:11,  2.42it/s, loss=2.422, v_num=6664329]Epoch 1:  12%|█▏        | 24/196 [00:09<01:10,  2.42it/s, loss=2.422, v_num=6664329]Epoch 1:  12%|█▏        | 24/196 [00:09<01:10,  2.42it/s, loss=2.357, v_num=6664329]Epoch 1:  13%|█▎        | 25/196 [00:10<01:10,  2.43it/s, loss=2.357, v_num=6664329]Epoch 1:  13%|█▎        | 25/196 [00:10<01:10,  2.43it/s, loss=2.306, v_num=6664329]Epoch 1:  13%|█▎        | 26/196 [00:10<01:09,  2.44it/s, loss=2.306, v_num=6664329]Epoch 1:  13%|█▎        | 26/196 [00:10<01:09,  2.44it/s, loss=2.266, v_num=6664329]Epoch 1:  14%|█▍        | 27/196 [00:11<01:08,  2.45it/s, loss=2.266, v_num=6664329]Epoch 1:  14%|█▍        | 27/196 [00:11<01:08,  2.45it/s, loss=2.238, v_num=6664329]Epoch 1:  14%|█▍        | 28/196 [00:11<01:08,  2.46it/s, loss=2.238, v_num=6664329]Epoch 1:  14%|█▍        | 28/196 [00:11<01:08,  2.46it/s, loss=2.220, v_num=6664329]Epoch 1:  15%|█▍        | 29/196 [00:11<01:07,  2.46it/s, loss=2.220, v_num=6664329]Epoch 1:  15%|█▍        | 29/196 [00:11<01:07,  2.46it/s, loss=2.215, v_num=6664329]Epoch 1:  15%|█▌        | 30/196 [00:12<01:07,  2.47it/s, loss=2.215, v_num=6664329]Epoch 1:  15%|█▌        | 30/196 [00:12<01:07,  2.47it/s, loss=2.212, v_num=6664329]Epoch 1:  16%|█▌        | 31/196 [00:12<01:06,  2.47it/s, loss=2.212, v_num=6664329]Epoch 1:  16%|█▌        | 31/196 [00:12<01:06,  2.47it/s, loss=2.205, v_num=6664329]Epoch 1:  16%|█▋        | 32/196 [00:12<01:06,  2.48it/s, loss=2.205, v_num=6664329]Epoch 1:  16%|█▋        | 32/196 [00:12<01:06,  2.48it/s, loss=2.197, v_num=6664329]Epoch 1:  17%|█▋        | 33/196 [00:13<01:05,  2.49it/s, loss=2.197, v_num=6664329]Epoch 1:  17%|█▋        | 33/196 [00:13<01:05,  2.49it/s, loss=2.192, v_num=6664329]Epoch 1:  17%|█▋        | 34/196 [00:13<01:05,  2.49it/s, loss=2.192, v_num=6664329]Epoch 1:  17%|█▋        | 34/196 [00:13<01:05,  2.49it/s, loss=2.181, v_num=6664329]Epoch 1:  18%|█▊        | 35/196 [00:14<01:04,  2.50it/s, loss=2.181, v_num=6664329]Epoch 1:  18%|█▊        | 35/196 [00:14<01:04,  2.50it/s, loss=2.176, v_num=6664329]Epoch 1:  18%|█▊        | 36/196 [00:14<01:03,  2.50it/s, loss=2.176, v_num=6664329]Epoch 1:  18%|█▊        | 36/196 [00:14<01:03,  2.50it/s, loss=2.173, v_num=6664329]Epoch 1:  19%|█▉        | 37/196 [00:14<01:03,  2.51it/s, loss=2.173, v_num=6664329]Epoch 1:  19%|█▉        | 37/196 [00:14<01:03,  2.51it/s, loss=2.171, v_num=6664329]Epoch 1:  19%|█▉        | 38/196 [00:15<01:02,  2.51it/s, loss=2.171, v_num=6664329]Epoch 1:  19%|█▉        | 38/196 [00:15<01:02,  2.51it/s, loss=2.166, v_num=6664329]Epoch 1:  20%|█▉        | 39/196 [00:15<01:02,  2.51it/s, loss=2.166, v_num=6664329]Epoch 1:  20%|█▉        | 39/196 [00:15<01:02,  2.51it/s, loss=2.160, v_num=6664329]Epoch 1:  20%|██        | 40/196 [00:15<01:01,  2.52it/s, loss=2.160, v_num=6664329]Epoch 1:  20%|██        | 40/196 [00:15<01:01,  2.52it/s, loss=2.157, v_num=6664329]Epoch 1:  21%|██        | 41/196 [00:16<01:01,  2.52it/s, loss=2.157, v_num=6664329]Epoch 1:  21%|██        | 41/196 [00:16<01:01,  2.52it/s, loss=2.151, v_num=6664329]Epoch 1:  21%|██▏       | 42/196 [00:16<01:01,  2.52it/s, loss=2.151, v_num=6664329]Epoch 1:  21%|██▏       | 42/196 [00:16<01:01,  2.52it/s, loss=2.147, v_num=6664329]Epoch 1:  22%|██▏       | 43/196 [00:17<01:00,  2.53it/s, loss=2.147, v_num=6664329]Epoch 1:  22%|██▏       | 43/196 [00:17<01:00,  2.53it/s, loss=2.144, v_num=6664329]Epoch 1:  22%|██▏       | 44/196 [00:17<01:00,  2.53it/s, loss=2.144, v_num=6664329]Epoch 1:  22%|██▏       | 44/196 [00:17<01:00,  2.53it/s, loss=2.144, v_num=6664329]Epoch 1:  23%|██▎       | 45/196 [00:17<00:59,  2.53it/s, loss=2.144, v_num=6664329]Epoch 1:  23%|██▎       | 45/196 [00:17<00:59,  2.53it/s, loss=2.145, v_num=6664329]Epoch 1:  23%|██▎       | 46/196 [00:18<00:59,  2.54it/s, loss=2.145, v_num=6664329]Epoch 1:  23%|██▎       | 46/196 [00:18<00:59,  2.54it/s, loss=2.146, v_num=6664329]Epoch 1:  24%|██▍       | 47/196 [00:18<00:58,  2.54it/s, loss=2.146, v_num=6664329]Epoch 1:  24%|██▍       | 47/196 [00:18<00:58,  2.54it/s, loss=2.147, v_num=6664329]Epoch 1:  24%|██▍       | 48/196 [00:18<00:58,  2.54it/s, loss=2.147, v_num=6664329]Epoch 1:  24%|██▍       | 48/196 [00:18<00:58,  2.54it/s, loss=2.145, v_num=6664329]Epoch 1:  25%|██▌       | 49/196 [00:19<00:57,  2.55it/s, loss=2.145, v_num=6664329]Epoch 1:  25%|██▌       | 49/196 [00:19<00:57,  2.55it/s, loss=2.139, v_num=6664329]Epoch 1:  26%|██▌       | 50/196 [00:19<00:57,  2.55it/s, loss=2.139, v_num=6664329]Epoch 1:  26%|██▌       | 50/196 [00:19<00:57,  2.55it/s, loss=2.133, v_num=6664329]Epoch 1:  26%|██▌       | 51/196 [00:19<00:56,  2.55it/s, loss=2.133, v_num=6664329]Epoch 1:  26%|██▌       | 51/196 [00:19<00:56,  2.55it/s, loss=2.133, v_num=6664329]Epoch 1:  27%|██▋       | 52/196 [00:20<00:56,  2.55it/s, loss=2.133, v_num=6664329]Epoch 1:  27%|██▋       | 52/196 [00:20<00:56,  2.55it/s, loss=2.124, v_num=6664329]Epoch 1:  27%|██▋       | 53/196 [00:20<00:55,  2.56it/s, loss=2.124, v_num=6664329]Epoch 1:  27%|██▋       | 53/196 [00:20<00:55,  2.56it/s, loss=2.121, v_num=6664329]Epoch 1:  28%|██▊       | 54/196 [00:21<00:55,  2.56it/s, loss=2.121, v_num=6664329]Epoch 1:  28%|██▊       | 54/196 [00:21<00:55,  2.56it/s, loss=2.121, v_num=6664329]Epoch 1:  28%|██▊       | 55/196 [00:21<00:55,  2.56it/s, loss=2.121, v_num=6664329]Epoch 1:  28%|██▊       | 55/196 [00:21<00:55,  2.56it/s, loss=2.119, v_num=6664329]Epoch 1:  29%|██▊       | 56/196 [00:21<00:54,  2.56it/s, loss=2.119, v_num=6664329]Epoch 1:  29%|██▊       | 56/196 [00:21<00:54,  2.56it/s, loss=2.112, v_num=6664329]Epoch 1:  29%|██▉       | 57/196 [00:22<00:54,  2.56it/s, loss=2.112, v_num=6664329]Epoch 1:  29%|██▉       | 57/196 [00:22<00:54,  2.56it/s, loss=2.113, v_num=6664329]Epoch 1:  30%|██▉       | 58/196 [00:22<00:53,  2.56it/s, loss=2.113, v_num=6664329]Epoch 1:  30%|██▉       | 58/196 [00:22<00:53,  2.56it/s, loss=2.108, v_num=6664329]Epoch 1:  30%|███       | 59/196 [00:22<00:53,  2.57it/s, loss=2.108, v_num=6664329]Epoch 1:  30%|███       | 59/196 [00:22<00:53,  2.57it/s, loss=2.110, v_num=6664329]Epoch 1:  31%|███       | 60/196 [00:23<00:52,  2.57it/s, loss=2.110, v_num=6664329]Epoch 1:  31%|███       | 60/196 [00:23<00:52,  2.57it/s, loss=2.104, v_num=6664329]Epoch 1:  31%|███       | 61/196 [00:23<00:52,  2.57it/s, loss=2.104, v_num=6664329]Epoch 1:  31%|███       | 61/196 [00:23<00:52,  2.57it/s, loss=2.104, v_num=6664329]Epoch 1:  32%|███▏      | 62/196 [00:24<00:52,  2.57it/s, loss=2.104, v_num=6664329]Epoch 1:  32%|███▏      | 62/196 [00:24<00:52,  2.57it/s, loss=2.101, v_num=6664329]Epoch 1:  32%|███▏      | 63/196 [00:24<00:51,  2.57it/s, loss=2.101, v_num=6664329]Epoch 1:  32%|███▏      | 63/196 [00:24<00:51,  2.57it/s, loss=2.096, v_num=6664329]Epoch 1:  33%|███▎      | 64/196 [00:24<00:51,  2.58it/s, loss=2.096, v_num=6664329]Epoch 1:  33%|███▎      | 64/196 [00:24<00:51,  2.58it/s, loss=2.091, v_num=6664329]Epoch 1:  33%|███▎      | 65/196 [00:25<00:50,  2.58it/s, loss=2.091, v_num=6664329]Epoch 1:  33%|███▎      | 65/196 [00:25<00:50,  2.58it/s, loss=2.090, v_num=6664329]Epoch 1:  34%|███▎      | 66/196 [00:25<00:50,  2.58it/s, loss=2.090, v_num=6664329]Epoch 1:  34%|███▎      | 66/196 [00:25<00:50,  2.58it/s, loss=2.085, v_num=6664329]Epoch 1:  34%|███▍      | 67/196 [00:25<00:49,  2.58it/s, loss=2.085, v_num=6664329]Epoch 1:  34%|███▍      | 67/196 [00:25<00:49,  2.58it/s, loss=2.083, v_num=6664329]Epoch 1:  35%|███▍      | 68/196 [00:26<00:49,  2.58it/s, loss=2.083, v_num=6664329]Epoch 1:  35%|███▍      | 68/196 [00:26<00:49,  2.58it/s, loss=2.079, v_num=6664329]Epoch 1:  35%|███▌      | 69/196 [00:26<00:49,  2.58it/s, loss=2.079, v_num=6664329]Epoch 1:  35%|███▌      | 69/196 [00:26<00:49,  2.58it/s, loss=2.078, v_num=6664329]Epoch 1:  36%|███▌      | 70/196 [00:27<00:48,  2.58it/s, loss=2.078, v_num=6664329]Epoch 1:  36%|███▌      | 70/196 [00:27<00:48,  2.58it/s, loss=2.077, v_num=6664329]Epoch 1:  36%|███▌      | 71/196 [00:27<00:48,  2.58it/s, loss=2.077, v_num=6664329]Epoch 1:  36%|███▌      | 71/196 [00:27<00:48,  2.58it/s, loss=2.074, v_num=6664329]Epoch 1:  37%|███▋      | 72/196 [00:27<00:47,  2.59it/s, loss=2.074, v_num=6664329]Epoch 1:  37%|███▋      | 72/196 [00:27<00:47,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  37%|███▋      | 73/196 [00:28<00:47,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  37%|███▋      | 73/196 [00:28<00:47,  2.59it/s, loss=2.081, v_num=6664329]Epoch 1:  38%|███▊      | 74/196 [00:28<00:47,  2.59it/s, loss=2.081, v_num=6664329]Epoch 1:  38%|███▊      | 74/196 [00:28<00:47,  2.59it/s, loss=2.081, v_num=6664329]Epoch 1:  38%|███▊      | 75/196 [00:28<00:46,  2.59it/s, loss=2.081, v_num=6664329]Epoch 1:  38%|███▊      | 75/196 [00:28<00:46,  2.59it/s, loss=2.081, v_num=6664329]Epoch 1:  39%|███▉      | 76/196 [00:29<00:46,  2.59it/s, loss=2.081, v_num=6664329]Epoch 1:  39%|███▉      | 76/196 [00:29<00:46,  2.59it/s, loss=2.082, v_num=6664329]Epoch 1:  39%|███▉      | 77/196 [00:29<00:45,  2.59it/s, loss=2.082, v_num=6664329]Epoch 1:  39%|███▉      | 77/196 [00:29<00:45,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  40%|███▉      | 78/196 [00:30<00:45,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  40%|███▉      | 78/196 [00:30<00:45,  2.59it/s, loss=2.079, v_num=6664329]Epoch 1:  40%|████      | 79/196 [00:30<00:45,  2.59it/s, loss=2.079, v_num=6664329]Epoch 1:  40%|████      | 79/196 [00:30<00:45,  2.59it/s, loss=2.073, v_num=6664329]Epoch 1:  41%|████      | 80/196 [00:30<00:44,  2.59it/s, loss=2.073, v_num=6664329]Epoch 1:  41%|████      | 80/196 [00:30<00:44,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  41%|████▏     | 81/196 [00:31<00:44,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  41%|████▏     | 81/196 [00:31<00:44,  2.59it/s, loss=2.075, v_num=6664329]Epoch 1:  42%|████▏     | 82/196 [00:31<00:43,  2.59it/s, loss=2.075, v_num=6664329]Epoch 1:  42%|████▏     | 82/196 [00:31<00:43,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  42%|████▏     | 83/196 [00:31<00:43,  2.59it/s, loss=2.078, v_num=6664329]Epoch 1:  42%|████▏     | 83/196 [00:31<00:43,  2.59it/s, loss=2.079, v_num=6664329]Epoch 1:  43%|████▎     | 84/196 [00:32<00:43,  2.59it/s, loss=2.079, v_num=6664329]Epoch 1:  43%|████▎     | 84/196 [00:32<00:43,  2.59it/s, loss=2.082, v_num=6664329]Epoch 1:  43%|████▎     | 85/196 [00:32<00:42,  2.59it/s, loss=2.082, v_num=6664329]Epoch 1:  43%|████▎     | 85/196 [00:32<00:42,  2.59it/s, loss=2.074, v_num=6664329]Epoch 1:  44%|████▍     | 86/196 [00:33<00:42,  2.60it/s, loss=2.074, v_num=6664329]Epoch 1:  44%|████▍     | 86/196 [00:33<00:42,  2.60it/s, loss=2.076, v_num=6664329]Epoch 1:  44%|████▍     | 87/196 [00:33<00:41,  2.60it/s, loss=2.076, v_num=6664329]Epoch 1:  44%|████▍     | 87/196 [00:33<00:41,  2.60it/s, loss=2.072, v_num=6664329]Epoch 1:  45%|████▍     | 88/196 [00:33<00:41,  2.60it/s, loss=2.072, v_num=6664329]Epoch 1:  45%|████▍     | 88/196 [00:33<00:41,  2.60it/s, loss=2.074, v_num=6664329]Epoch 1:  45%|████▌     | 89/196 [00:34<00:41,  2.60it/s, loss=2.074, v_num=6664329]Epoch 1:  45%|████▌     | 89/196 [00:34<00:41,  2.60it/s, loss=2.074, v_num=6664329]Epoch 1:  46%|████▌     | 90/196 [00:34<00:40,  2.60it/s, loss=2.074, v_num=6664329]Epoch 1:  46%|████▌     | 90/196 [00:34<00:40,  2.60it/s, loss=2.069, v_num=6664329]Epoch 1:  46%|████▋     | 91/196 [00:35<00:40,  2.60it/s, loss=2.069, v_num=6664329]Epoch 1:  46%|████▋     | 91/196 [00:35<00:40,  2.60it/s, loss=2.073, v_num=6664329]Epoch 1:  47%|████▋     | 92/196 [00:35<00:40,  2.60it/s, loss=2.073, v_num=6664329]Epoch 1:  47%|████▋     | 92/196 [00:35<00:40,  2.60it/s, loss=2.072, v_num=6664329]Epoch 1:  47%|████▋     | 93/196 [00:35<00:39,  2.60it/s, loss=2.072, v_num=6664329]Epoch 1:  47%|████▋     | 93/196 [00:35<00:39,  2.60it/s, loss=2.065, v_num=6664329]Epoch 1:  48%|████▊     | 94/196 [00:36<00:39,  2.60it/s, loss=2.065, v_num=6664329]Epoch 1:  48%|████▊     | 94/196 [00:36<00:39,  2.60it/s, loss=2.064, v_num=6664329]Epoch 1:  48%|████▊     | 95/196 [00:36<00:38,  2.60it/s, loss=2.064, v_num=6664329]Epoch 1:  48%|████▊     | 95/196 [00:36<00:38,  2.60it/s, loss=2.062, v_num=6664329]Epoch 1:  49%|████▉     | 96/196 [00:36<00:38,  2.60it/s, loss=2.062, v_num=6664329]Epoch 1:  49%|████▉     | 96/196 [00:36<00:38,  2.60it/s, loss=2.056, v_num=6664329]Epoch 1:  49%|████▉     | 97/196 [00:37<00:38,  2.60it/s, loss=2.056, v_num=6664329]Epoch 1:  49%|████▉     | 97/196 [00:37<00:38,  2.60it/s, loss=2.051, v_num=6664329]Epoch 1:  50%|█████     | 98/196 [00:37<00:37,  2.60it/s, loss=2.051, v_num=6664329]Epoch 1:  50%|█████     | 98/196 [00:37<00:37,  2.60it/s, loss=2.047, v_num=6664329]Epoch 1:  51%|█████     | 99/196 [00:38<00:37,  2.60it/s, loss=2.047, v_num=6664329]Epoch 1:  51%|█████     | 99/196 [00:38<00:37,  2.60it/s, loss=2.043, v_num=6664329]Epoch 1:  51%|█████     | 100/196 [00:38<00:36,  2.60it/s, loss=2.043, v_num=6664329]Epoch 1:  51%|█████     | 100/196 [00:38<00:36,  2.60it/s, loss=2.032, v_num=6664329]Epoch 1:  52%|█████▏    | 101/196 [00:38<00:36,  2.60it/s, loss=2.032, v_num=6664329]Epoch 1:  52%|█████▏    | 101/196 [00:38<00:36,  2.60it/s, loss=2.027, v_num=6664329]Epoch 1:  52%|█████▏    | 102/196 [00:39<00:36,  2.61it/s, loss=2.027, v_num=6664329]Epoch 1:  52%|█████▏    | 102/196 [00:39<00:36,  2.61it/s, loss=2.017, v_num=6664329]Epoch 1:  53%|█████▎    | 103/196 [00:39<00:35,  2.61it/s, loss=2.017, v_num=6664329]Epoch 1:  53%|█████▎    | 103/196 [00:39<00:35,  2.61it/s, loss=2.012, v_num=6664329]Epoch 1:  53%|█████▎    | 104/196 [00:39<00:35,  2.61it/s, loss=2.012, v_num=6664329]Epoch 1:  53%|█████▎    | 104/196 [00:39<00:35,  2.61it/s, loss=2.006, v_num=6664329]Epoch 1:  54%|█████▎    | 105/196 [00:40<00:34,  2.61it/s, loss=2.006, v_num=6664329]Epoch 1:  54%|█████▎    | 105/196 [00:40<00:34,  2.61it/s, loss=2.005, v_num=6664329]Epoch 1:  54%|█████▍    | 106/196 [00:40<00:34,  2.61it/s, loss=2.005, v_num=6664329]Epoch 1:  54%|█████▍    | 106/196 [00:40<00:34,  2.61it/s, loss=2.002, v_num=6664329]Epoch 1:  55%|█████▍    | 107/196 [00:41<00:34,  2.61it/s, loss=2.002, v_num=6664329]Epoch 1:  55%|█████▍    | 107/196 [00:41<00:34,  2.61it/s, loss=1.998, v_num=6664329]Epoch 1:  55%|█████▌    | 108/196 [00:41<00:33,  2.61it/s, loss=1.998, v_num=6664329]Epoch 1:  55%|█████▌    | 108/196 [00:41<00:33,  2.61it/s, loss=1.991, v_num=6664329]Epoch 1:  56%|█████▌    | 109/196 [00:41<00:33,  2.61it/s, loss=1.991, v_num=6664329]Epoch 1:  56%|█████▌    | 109/196 [00:41<00:33,  2.61it/s, loss=1.988, v_num=6664329]Epoch 1:  56%|█████▌    | 110/196 [00:42<00:32,  2.61it/s, loss=1.988, v_num=6664329]Epoch 1:  56%|█████▌    | 110/196 [00:42<00:32,  2.61it/s, loss=1.989, v_num=6664329]Epoch 1:  57%|█████▋    | 111/196 [00:42<00:32,  2.61it/s, loss=1.989, v_num=6664329]Epoch 1:  57%|█████▋    | 111/196 [00:42<00:32,  2.61it/s, loss=1.978, v_num=6664329]Epoch 1:  57%|█████▋    | 112/196 [00:42<00:32,  2.61it/s, loss=1.978, v_num=6664329]Epoch 1:  57%|█████▋    | 112/196 [00:42<00:32,  2.61it/s, loss=1.969, v_num=6664329]Epoch 1:  58%|█████▊    | 113/196 [00:43<00:31,  2.61it/s, loss=1.969, v_num=6664329]Epoch 1:  58%|█████▊    | 113/196 [00:43<00:31,  2.61it/s, loss=1.970, v_num=6664329]Epoch 1:  58%|█████▊    | 114/196 [00:43<00:31,  2.61it/s, loss=1.970, v_num=6664329]Epoch 1:  58%|█████▊    | 114/196 [00:43<00:31,  2.61it/s, loss=1.964, v_num=6664329]Epoch 1:  59%|█████▊    | 115/196 [00:44<00:31,  2.61it/s, loss=1.964, v_num=6664329]Epoch 1:  59%|█████▊    | 115/196 [00:44<00:31,  2.61it/s, loss=1.959, v_num=6664329]Epoch 1:  59%|█████▉    | 116/196 [00:44<00:30,  2.61it/s, loss=1.959, v_num=6664329]Epoch 1:  59%|█████▉    | 116/196 [00:44<00:30,  2.61it/s, loss=1.959, v_num=6664329]Epoch 1:  60%|█████▉    | 117/196 [00:44<00:30,  2.61it/s, loss=1.959, v_num=6664329]Epoch 1:  60%|█████▉    | 117/196 [00:44<00:30,  2.61it/s, loss=1.954, v_num=6664329]Epoch 1:  60%|██████    | 118/196 [00:45<00:29,  2.61it/s, loss=1.954, v_num=6664329]Epoch 1:  60%|██████    | 118/196 [00:45<00:29,  2.61it/s, loss=1.954, v_num=6664329]Epoch 1:  61%|██████    | 119/196 [00:45<00:29,  2.61it/s, loss=1.954, v_num=6664329]Epoch 1:  61%|██████    | 119/196 [00:45<00:29,  2.61it/s, loss=1.950, v_num=6664329]Epoch 1:  61%|██████    | 120/196 [00:45<00:29,  2.61it/s, loss=1.950, v_num=6664329]Epoch 1:  61%|██████    | 120/196 [00:45<00:29,  2.61it/s, loss=1.951, v_num=6664329]Epoch 1:  62%|██████▏   | 121/196 [00:46<00:28,  2.61it/s, loss=1.951, v_num=6664329]Epoch 1:  62%|██████▏   | 121/196 [00:46<00:28,  2.61it/s, loss=1.948, v_num=6664329]Epoch 1:  62%|██████▏   | 122/196 [00:46<00:28,  2.61it/s, loss=1.948, v_num=6664329]Epoch 1:  62%|██████▏   | 122/196 [00:46<00:28,  2.61it/s, loss=1.945, v_num=6664329]Epoch 1:  63%|██████▎   | 123/196 [00:47<00:27,  2.61it/s, loss=1.945, v_num=6664329]Epoch 1:  63%|██████▎   | 123/196 [00:47<00:27,  2.61it/s, loss=1.940, v_num=6664329]Epoch 1:  63%|██████▎   | 124/196 [00:47<00:27,  2.61it/s, loss=1.940, v_num=6664329]Epoch 1:  63%|██████▎   | 124/196 [00:47<00:27,  2.61it/s, loss=1.937, v_num=6664329]slurmstepd: error: *** JOB 6664329 ON gr001-ib0 CANCELLED AT 2021-05-10T12:31:41 ***
