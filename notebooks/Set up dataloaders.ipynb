{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bridal-commodity",
   "metadata": {},
   "source": [
    "Instantiate train, valid, and test dataloaders for each of the three datasets\n",
    "\n",
    "To turn the torch datasets into a validation split as well, following the instructions at\n",
    "https://medium.com/@sergioalves94/deep-learning-in-pytorch-with-cifar-10-dataset-858b504a6b54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sonic-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../vit_pytorch/')\n",
    "sys.path.append('../')\n",
    "\n",
    "from vit import ViT\n",
    "from recorder import Recorder # import the Recorder and instantiate\n",
    "\n",
    "#from vit_pytorch.efficient import ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-thermal",
   "metadata": {},
   "source": [
    "# CIFAR 10 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-bikini",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coordinate-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR_data(number='10',\n",
    "                   val_size = 5000,\n",
    "                   batch_size = 64,\n",
    "                   transforms=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                                   ])):\n",
    "\n",
    "    if number == '10':\n",
    "        dataset = datasets.CIFAR10(root='../data/', download=True, transform=transforms)\n",
    "        test_dataset = datasets.CIFAR10(root='../data/', train=False, transform=transforms)\n",
    "    elif number == '100': \n",
    "        dataset = datasets.CIFAR100(root='../data/', download=True, transform=transforms)\n",
    "        test_dataset = datasets.CIFAR100(root='../data/', train=False, transform=transforms)\n",
    "        \n",
    "    else:\n",
    "        print(\"Must select 10 or 100\")\n",
    "        sys.exit()\n",
    "        \n",
    "        \n",
    "\n",
    "    train_size = len(dataset) - val_size \n",
    "\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "special-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/nsk367/anaconda3/envs/vit/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_CIFAR_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = datasets.CIFAR10(root='../data/', download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='../data/', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dataset.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = {}\n",
    "for _, index in dataset:\n",
    "    label = classes[index]\n",
    "    if label not in class_count:\n",
    "        class_count[label] = 0\n",
    "    class_count[label] += 1\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in train_loader:\n",
    "    print('images.shape:', images.shape)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-destination",
   "metadata": {},
   "source": [
    "And this is basically all we need to do for these torchvision datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-television",
   "metadata": {},
   "source": [
    "# CIFAR100\n",
    "\n",
    "\n",
    "Basically copy and pasting above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = datasets.CIFAR100(root='../data/', download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR100(root='../data/', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-leeds",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = dataset.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-simon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_count = {}\n",
    "for _, index in dataset:\n",
    "    label = classes[index]\n",
    "    if label not in class_count:\n",
    "        class_count[label] = 0\n",
    "    class_count[label] += 1\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-folks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Proof still balanced after splitting.\")\n",
    "class_count = {}\n",
    "for _, index in val_ds:\n",
    "    label = classes[index]\n",
    "    if label not in class_count:\n",
    "        class_count[label] = 0\n",
    "    class_count[label] += 1\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in train_loader:\n",
    "    print('images.shape:', images.shape)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-accent",
   "metadata": {},
   "source": [
    "# Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-macro",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
