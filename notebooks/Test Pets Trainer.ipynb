{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly debugging the ViT trainer code before doing some test runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_lightning import Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import cv2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../vit_pytorch/')\n",
    "sys.path.append('..')\n",
    "from vit import ViT\n",
    "from recorder import Recorder # import the Recorder and instantiate\n",
    "#from dataloaders import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from dataloader import get_CIFAR_data\n",
    "from pets_loader import get_PETS_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT_Trainer(pl.LightningModule):\n",
    "    def __init__(self, hparams=None):\n",
    "        super(ViT_Trainer,self).__init__()\n",
    "        self.__check_hparams(hparams)\n",
    "        self.hparams = hparams\n",
    "        self.prepare_data()\n",
    "\n",
    "        self.__model = ViT(\n",
    "                            dim=self.dim,\n",
    "                            image_size=self.image_size,\n",
    "                            patch_size=self.patch_size,\n",
    "                            num_classes=self.num_classes,\n",
    "                            channels=self.channels,\n",
    "                            depth = self.depth,\n",
    "                            heads=self.heads, \n",
    "                            mlp_dim=self.mlp_dim,\n",
    "                            dropout=self.dropout\n",
    "                        )\n",
    "        self.rec = Recorder()\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        y_pred = self.__model(x,rec = self.rec)# returns the predicted class for this dataset. \n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def _run_step(self, batch, batch_idx,step_name):\n",
    "\n",
    "        img, y_true  = batch\n",
    "        y_pred = self(img) \n",
    "\n",
    "        if batch_idx % 1500 == 0:\n",
    "            # log progress. save a few images from the batch, what they are, and what their prediction is. \n",
    "            self.__log_step(img,y_true,y_pred, step_name)\n",
    "\n",
    "\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, y_true)\n",
    "\n",
    "        return loss , y_pred, y_true\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        train_loss, _, _ = self._run_step( batch, batch_idx,step_name='train') \n",
    "        train_tensorboard_logs = {'train_loss': train_loss}\n",
    "        \n",
    "        return {'loss': train_loss, 'log': train_tensorboard_logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        val_log_dict = {}\n",
    "        val_loss, y_pred, y_true = self._run_step(batch, batch_idx, step_name='valid')\n",
    "        y_pred = y_pred.argmax(dim=1).detach().cpu()\n",
    "        y_true = y_true.detach().cpu()\n",
    "        val_log_dict['val_loss'] = val_loss\n",
    "        val_acc = torch.from_numpy(np.array([accuracy_score(y_pred,y_true)]))\n",
    "        val_log_dict['val_acc'] = val_acc\n",
    "\n",
    "        return val_log_dict \n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):   \n",
    "\n",
    "        val_tensorboard_logs = {}\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()  \n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        val_tensorboard_logs['avg_val_acc'] = avg_val_acc\n",
    "        val_tensorboard_logs['avg_val_loss'] = avg_val_loss\n",
    "\n",
    "        return {'val_loss': avg_val_loss, 'log': val_tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        test_log_dict = {}\n",
    "        test_loss, y_pred, y_true = self._run_step(batch, batch_idx, step_name='test')\n",
    "        y_pred = y_pred.argmax(dim=1).detach().cpu()\n",
    "        y_true = y_true.detach().cpu()\n",
    "        test_log_dict['test_loss'] = test_loss\n",
    "        test_acc = torch.from_numpy(np.array([accuracy_score(y_pred,y_true)]))\n",
    "        test_log_dict['test_acc'] = test_acc\n",
    "\n",
    "        return test_log_dict \n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outputs):    \n",
    "\n",
    "        test_tensorboard_logs = {}\n",
    "        avg_test_loss = torch.stack([x['test_loss'] for x in outputs]).mean()  \n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        test_tensorboard_logs['avg_test_acc'] = avg_test_acc\n",
    "        test_tensorboard_logs['avg_test_loss'] = avg_test_loss\n",
    "\n",
    "        return {'test_loss': avg_test_loss, 'log': test_tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  torch.optim.Adam(self.parameters(), lr = self.learning_rate ,weight_decay = self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience = 4)\n",
    "        return [optimizer], [scheduler] \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # the dataloaders are run batch by batch where this is run fully and once before beginning training\n",
    "#         self.train_loader, self.valid_loader, self.test_loader = get_CIFAR_data(batch_size=self.batch_size,\n",
    "#                                                                                  dset = self.dataset, \n",
    "#                                                                                  )\n",
    "        self.train_loader, self.valid_loader, self.test_loader = get_PETS_data(batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "    def __log_step(self,img, y_true, y_pred, step_name, limit=1):\n",
    "        ## Plot attention map \n",
    "        j = 0 # using the jth element from that batch \n",
    "        attn_mat = self.rec.attn[j].cpu()\n",
    "        im = img[j].cpu().numpy().transpose(1,2,0)\n",
    "        attn_mat = torch.mean(attn_mat, dim=1) # average across heads \n",
    "        # To account for residual connections, we add an identity matrix to the\n",
    "        # attention matrix and re-normalize the weights.\n",
    "        residual_att = torch.eye(attn_mat.size(1))\n",
    "        aug_att_mat = attn_mat + residual_att\n",
    "        aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "        # Recursively multiply the weight matrices\n",
    "        joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "        joint_attentions[0] = aug_att_mat[0]\n",
    "        for n in range(1, aug_att_mat.size(0)):\n",
    "            joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "            \n",
    "        # combines all the different layers which apply attention. \n",
    "\n",
    "        # Attention from the output token to the input space.\n",
    "        v = joint_attentions[-1]\n",
    "        grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "        mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "        mask = cv2.resize(mask / mask.max(), (self.image_size,self.image_size))[..., np.newaxis]\n",
    "        result = (mask * im.astype(\"uint8\"))\n",
    "        #TODO \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(im) #grayscale\n",
    "        tag = f'{step_name}_image'\n",
    "        self.logger.experiment.add_figure(tag, fig, global_step=self.trainer.global_step, close=True, walltime=None)\n",
    "          \n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(mask)\n",
    "        tag = f'{step_name}_attention_mask'\n",
    "        self.logger.experiment.add_figure(tag, fig, global_step=self.trainer.global_step, close=True, walltime=None)\n",
    "\n",
    "        \n",
    "    \n",
    "    def __check_hparams(self, hparams):\n",
    "        self.channels = hparams.channels if hasattr(hparams, 'channels') else 3\n",
    "        self.image_size = hparams.image_size if hasattr(hparams, 'image_size') else 32\n",
    "        self.patch_size = hparams.patch_size if hasattr(hparams, 'patch_size') else 8\n",
    "        self.depth = hparams.depth if hasattr(hparams, 'depth') else 8\n",
    "        self.heads = hparams.heads if hasattr(hparams, 'heads') else 8\n",
    "        self.dim = hparams.dim if hasattr(hparams, 'dim') else 768\n",
    "        self.mlp_dim = hparams.mlp_dim if hasattr(hparams, 'mlp_dim') else 512\n",
    "        self.dropout = hparams.dropout if hasattr(hparams, 'dropout') else 0\n",
    "        self.num_classes = hparams.num_classes if hasattr(hparams, 'num_classes') else 100\n",
    "\n",
    "        self.batch_size = hparams.batch_size if hasattr(hparams, 'batch_size') else 128\n",
    "        self.learning_rate = hparams.learning_rate if hasattr(hparams, 'learning_rate') else 0.001\n",
    "        self.weight_decay = hparams.weight_decay if hasattr(hparams, 'weight_decay') else 0.001\n",
    "        self.seed = hparams.seed if hasattr(hparams, 'seed') else 32\n",
    "#         self.dataset = hparams.dataset if hasattr(hparams, 'dataset') else 'cifar100'\n",
    "        self.dataset = hparams.dataset if hasattr(hparams, 'dataset') else 'pets'\n",
    "#\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = HyperOptArgumentParser(parents=[parent_parser], add_help=False)\n",
    "\n",
    "        # architecture specific arguments\n",
    "        parser.add_argument('--channels', type=int, default=3) \n",
    "        parser.add_argument('--image_size', type=int, default=32)  \n",
    "        parser.add_argument('--patch_size', type=int, default=4)  # not really specified\n",
    "        parser.add_argument('--depth', type=int, default=12)  # 12, 24, 32\n",
    "        parser.add_argument('--heads', type=int, default=12)  # 12, 16, 16\n",
    "        parser.add_argument('--dim', type=int, default=768)  # 768, 1024, 1280\n",
    "        parser.add_argument('--mlp_dim', type=int, default=3072) # 3072, 4096, 5120\n",
    "        parser.add_argument('--dropout', type=float, default=0)  # 0 or .1\n",
    "        parser.add_argument('--num_classes', type=int, default=100) \n",
    "\n",
    "        # setup arguments\n",
    "        parser.add_argument('--batch_size', type=int, default=128)  # 4096 \n",
    "        parser.add_argument('--learning_rate', type=int, default=1e-4) # .9, .999 (Adam)\n",
    "        parser.add_argument('--weight_decay', type=int, default=.001) # .1\n",
    "        parser.add_argument('--seed', type=int, default = 42) # shuffling samples in data loader \n",
    "#         parser.add_argument('--dataset',type=str, default = 'cifar100') # which data set to train with. \n",
    "        parser.add_argument('--dataset',type=str, default = 'pets') # which data set to train with. \n",
    "\n",
    "        return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking for corrupted images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7704c1393f4141cb9f3a9e10b6607025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Abyssinian_34.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_139.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_145.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_167.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_177.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_191.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init our model\n",
    "model = ViT_Trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in model.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_pets in t:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking for corrupted images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28bd4376e3f4a15ab0945bec0dfbc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Abyssinian_34.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_139.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_145.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_167.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_177.jpg\n",
      "[INFO] Corrupted Image: ../data/oxford_iiit_pet/images/Egyptian_Mau_191.jpg\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "    | Name                                                   | Type        | Params\n",
      "-----------------------------------------------------------------------------------\n",
      "0   | _ViT_Trainer__model                                    | ViT         | 19 M  \n",
      "1   | _ViT_Trainer__model.to_patch_embedding                 | Sequential  | 148 K \n",
      "2   | _ViT_Trainer__model.to_patch_embedding.0               | Rearrange   | 0     \n",
      "3   | _ViT_Trainer__model.to_patch_embedding.1               | Linear      | 148 K \n",
      "4   | _ViT_Trainer__model.dropout                            | Dropout     | 0     \n",
      "5   | _ViT_Trainer__model.transformer                        | Transformer | 18 M  \n",
      "6   | _ViT_Trainer__model.transformer.layers                 | ModuleList  | 18 M  \n",
      "7   | _ViT_Trainer__model.transformer.layers.0               | ModuleList  | 2 M   \n",
      "8   | _ViT_Trainer__model.transformer.layers.0.0             | PreNorm     | 1 M   \n",
      "9   | _ViT_Trainer__model.transformer.layers.0.0.norm        | LayerNorm   | 1 K   \n",
      "10  | _ViT_Trainer__model.transformer.layers.0.0.fn          | Attention   | 1 M   \n",
      "11  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_qkv   | Linear      | 1 M   \n",
      "12  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out   | Sequential  | 393 K \n",
      "13  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out.0 | Linear      | 393 K \n",
      "14  | _ViT_Trainer__model.transformer.layers.0.0.fn.to_out.1 | Dropout     | 0     \n",
      "15  | _ViT_Trainer__model.transformer.layers.0.1             | PreNorm     | 789 K \n",
      "16  | _ViT_Trainer__model.transformer.layers.0.1.norm        | LayerNorm   | 1 K   \n",
      "17  | _ViT_Trainer__model.transformer.layers.0.1.fn          | FeedForward | 787 K \n",
      "18  | _ViT_Trainer__model.transformer.layers.0.1.fn.net      | Sequential  | 787 K \n",
      "19  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.0    | Linear      | 393 K \n",
      "20  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.1    | GELU        | 0     \n",
      "21  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.2    | Dropout     | 0     \n",
      "22  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.3    | Linear      | 393 K \n",
      "23  | _ViT_Trainer__model.transformer.layers.0.1.fn.net.4    | Dropout     | 0     \n",
      "24  | _ViT_Trainer__model.transformer.layers.1               | ModuleList  | 2 M   \n",
      "25  | _ViT_Trainer__model.transformer.layers.1.0             | PreNorm     | 1 M   \n",
      "26  | _ViT_Trainer__model.transformer.layers.1.0.norm        | LayerNorm   | 1 K   \n",
      "27  | _ViT_Trainer__model.transformer.layers.1.0.fn          | Attention   | 1 M   \n",
      "28  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_qkv   | Linear      | 1 M   \n",
      "29  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out   | Sequential  | 393 K \n",
      "30  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out.0 | Linear      | 393 K \n",
      "31  | _ViT_Trainer__model.transformer.layers.1.0.fn.to_out.1 | Dropout     | 0     \n",
      "32  | _ViT_Trainer__model.transformer.layers.1.1             | PreNorm     | 789 K \n",
      "33  | _ViT_Trainer__model.transformer.layers.1.1.norm        | LayerNorm   | 1 K   \n",
      "34  | _ViT_Trainer__model.transformer.layers.1.1.fn          | FeedForward | 787 K \n",
      "35  | _ViT_Trainer__model.transformer.layers.1.1.fn.net      | Sequential  | 787 K \n",
      "36  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.0    | Linear      | 393 K \n",
      "37  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.1    | GELU        | 0     \n",
      "38  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.2    | Dropout     | 0     \n",
      "39  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.3    | Linear      | 393 K \n",
      "40  | _ViT_Trainer__model.transformer.layers.1.1.fn.net.4    | Dropout     | 0     \n",
      "41  | _ViT_Trainer__model.transformer.layers.2               | ModuleList  | 2 M   \n",
      "42  | _ViT_Trainer__model.transformer.layers.2.0             | PreNorm     | 1 M   \n",
      "43  | _ViT_Trainer__model.transformer.layers.2.0.norm        | LayerNorm   | 1 K   \n",
      "44  | _ViT_Trainer__model.transformer.layers.2.0.fn          | Attention   | 1 M   \n",
      "45  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_qkv   | Linear      | 1 M   \n",
      "46  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out   | Sequential  | 393 K \n",
      "47  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out.0 | Linear      | 393 K \n",
      "48  | _ViT_Trainer__model.transformer.layers.2.0.fn.to_out.1 | Dropout     | 0     \n",
      "49  | _ViT_Trainer__model.transformer.layers.2.1             | PreNorm     | 789 K \n",
      "50  | _ViT_Trainer__model.transformer.layers.2.1.norm        | LayerNorm   | 1 K   \n",
      "51  | _ViT_Trainer__model.transformer.layers.2.1.fn          | FeedForward | 787 K \n",
      "52  | _ViT_Trainer__model.transformer.layers.2.1.fn.net      | Sequential  | 787 K \n",
      "53  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.0    | Linear      | 393 K \n",
      "54  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.1    | GELU        | 0     \n",
      "55  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.2    | Dropout     | 0     \n",
      "56  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.3    | Linear      | 393 K \n",
      "57  | _ViT_Trainer__model.transformer.layers.2.1.fn.net.4    | Dropout     | 0     \n",
      "58  | _ViT_Trainer__model.transformer.layers.3               | ModuleList  | 2 M   \n",
      "59  | _ViT_Trainer__model.transformer.layers.3.0             | PreNorm     | 1 M   \n",
      "60  | _ViT_Trainer__model.transformer.layers.3.0.norm        | LayerNorm   | 1 K   \n",
      "61  | _ViT_Trainer__model.transformer.layers.3.0.fn          | Attention   | 1 M   \n",
      "62  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_qkv   | Linear      | 1 M   \n",
      "63  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out   | Sequential  | 393 K \n",
      "64  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out.0 | Linear      | 393 K \n",
      "65  | _ViT_Trainer__model.transformer.layers.3.0.fn.to_out.1 | Dropout     | 0     \n",
      "66  | _ViT_Trainer__model.transformer.layers.3.1             | PreNorm     | 789 K \n",
      "67  | _ViT_Trainer__model.transformer.layers.3.1.norm        | LayerNorm   | 1 K   \n",
      "68  | _ViT_Trainer__model.transformer.layers.3.1.fn          | FeedForward | 787 K \n",
      "69  | _ViT_Trainer__model.transformer.layers.3.1.fn.net      | Sequential  | 787 K \n",
      "70  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.0    | Linear      | 393 K \n",
      "71  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.1    | GELU        | 0     \n",
      "72  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.2    | Dropout     | 0     \n",
      "73  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.3    | Linear      | 393 K \n",
      "74  | _ViT_Trainer__model.transformer.layers.3.1.fn.net.4    | Dropout     | 0     \n",
      "75  | _ViT_Trainer__model.transformer.layers.4               | ModuleList  | 2 M   \n",
      "76  | _ViT_Trainer__model.transformer.layers.4.0             | PreNorm     | 1 M   \n",
      "77  | _ViT_Trainer__model.transformer.layers.4.0.norm        | LayerNorm   | 1 K   \n",
      "78  | _ViT_Trainer__model.transformer.layers.4.0.fn          | Attention   | 1 M   \n",
      "79  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_qkv   | Linear      | 1 M   \n",
      "80  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out   | Sequential  | 393 K \n",
      "81  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out.0 | Linear      | 393 K \n",
      "82  | _ViT_Trainer__model.transformer.layers.4.0.fn.to_out.1 | Dropout     | 0     \n",
      "83  | _ViT_Trainer__model.transformer.layers.4.1             | PreNorm     | 789 K \n",
      "84  | _ViT_Trainer__model.transformer.layers.4.1.norm        | LayerNorm   | 1 K   \n",
      "85  | _ViT_Trainer__model.transformer.layers.4.1.fn          | FeedForward | 787 K \n",
      "86  | _ViT_Trainer__model.transformer.layers.4.1.fn.net      | Sequential  | 787 K \n",
      "87  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.0    | Linear      | 393 K \n",
      "88  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.1    | GELU        | 0     \n",
      "89  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.2    | Dropout     | 0     \n",
      "90  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.3    | Linear      | 393 K \n",
      "91  | _ViT_Trainer__model.transformer.layers.4.1.fn.net.4    | Dropout     | 0     \n",
      "92  | _ViT_Trainer__model.transformer.layers.5               | ModuleList  | 2 M   \n",
      "93  | _ViT_Trainer__model.transformer.layers.5.0             | PreNorm     | 1 M   \n",
      "94  | _ViT_Trainer__model.transformer.layers.5.0.norm        | LayerNorm   | 1 K   \n",
      "95  | _ViT_Trainer__model.transformer.layers.5.0.fn          | Attention   | 1 M   \n",
      "96  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_qkv   | Linear      | 1 M   \n",
      "97  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out   | Sequential  | 393 K \n",
      "98  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out.0 | Linear      | 393 K \n",
      "99  | _ViT_Trainer__model.transformer.layers.5.0.fn.to_out.1 | Dropout     | 0     \n",
      "100 | _ViT_Trainer__model.transformer.layers.5.1             | PreNorm     | 789 K \n",
      "101 | _ViT_Trainer__model.transformer.layers.5.1.norm        | LayerNorm   | 1 K   \n",
      "102 | _ViT_Trainer__model.transformer.layers.5.1.fn          | FeedForward | 787 K \n",
      "103 | _ViT_Trainer__model.transformer.layers.5.1.fn.net      | Sequential  | 787 K \n",
      "104 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.0    | Linear      | 393 K \n",
      "105 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.1    | GELU        | 0     \n",
      "106 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.2    | Dropout     | 0     \n",
      "107 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.3    | Linear      | 393 K \n",
      "108 | _ViT_Trainer__model.transformer.layers.5.1.fn.net.4    | Dropout     | 0     \n",
      "109 | _ViT_Trainer__model.transformer.layers.6               | ModuleList  | 2 M   \n",
      "110 | _ViT_Trainer__model.transformer.layers.6.0             | PreNorm     | 1 M   \n",
      "111 | _ViT_Trainer__model.transformer.layers.6.0.norm        | LayerNorm   | 1 K   \n",
      "112 | _ViT_Trainer__model.transformer.layers.6.0.fn          | Attention   | 1 M   \n",
      "113 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_qkv   | Linear      | 1 M   \n",
      "114 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out   | Sequential  | 393 K \n",
      "115 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out.0 | Linear      | 393 K \n",
      "116 | _ViT_Trainer__model.transformer.layers.6.0.fn.to_out.1 | Dropout     | 0     \n",
      "117 | _ViT_Trainer__model.transformer.layers.6.1             | PreNorm     | 789 K \n",
      "118 | _ViT_Trainer__model.transformer.layers.6.1.norm        | LayerNorm   | 1 K   \n",
      "119 | _ViT_Trainer__model.transformer.layers.6.1.fn          | FeedForward | 787 K \n",
      "120 | _ViT_Trainer__model.transformer.layers.6.1.fn.net      | Sequential  | 787 K \n",
      "121 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.0    | Linear      | 393 K \n",
      "122 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.1    | GELU        | 0     \n",
      "123 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.2    | Dropout     | 0     \n",
      "124 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.3    | Linear      | 393 K \n",
      "125 | _ViT_Trainer__model.transformer.layers.6.1.fn.net.4    | Dropout     | 0     \n",
      "126 | _ViT_Trainer__model.transformer.layers.7               | ModuleList  | 2 M   \n",
      "127 | _ViT_Trainer__model.transformer.layers.7.0             | PreNorm     | 1 M   \n",
      "128 | _ViT_Trainer__model.transformer.layers.7.0.norm        | LayerNorm   | 1 K   \n",
      "129 | _ViT_Trainer__model.transformer.layers.7.0.fn          | Attention   | 1 M   \n",
      "130 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_qkv   | Linear      | 1 M   \n",
      "131 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out   | Sequential  | 393 K \n",
      "132 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out.0 | Linear      | 393 K \n",
      "133 | _ViT_Trainer__model.transformer.layers.7.0.fn.to_out.1 | Dropout     | 0     \n",
      "134 | _ViT_Trainer__model.transformer.layers.7.1             | PreNorm     | 789 K \n",
      "135 | _ViT_Trainer__model.transformer.layers.7.1.norm        | LayerNorm   | 1 K   \n",
      "136 | _ViT_Trainer__model.transformer.layers.7.1.fn          | FeedForward | 787 K \n",
      "137 | _ViT_Trainer__model.transformer.layers.7.1.fn.net      | Sequential  | 787 K \n",
      "138 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.0    | Linear      | 393 K \n",
      "139 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.1    | GELU        | 0     \n",
      "140 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.2    | Dropout     | 0     \n",
      "141 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.3    | Linear      | 393 K \n",
      "142 | _ViT_Trainer__model.transformer.layers.7.1.fn.net.4    | Dropout     | 0     \n",
      "143 | _ViT_Trainer__model.to_latent                          | Identity    | 0     \n",
      "144 | _ViT_Trainer__model.mlp_head                           | Sequential  | 78 K  \n",
      "145 | _ViT_Trainer__model.mlp_head.0                         | LayerNorm   | 1 K   \n",
      "146 | _ViT_Trainer__model.mlp_head.1                         | Linear      | 76 K  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08c8c1d2b5e4781b53601a6cc5defb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22a14b12ba34677ab601d864bb30abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "INFO:lightning:Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=30, progress_bar_refresh_rate=20)\n",
    "\n",
    "# Train the model \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
